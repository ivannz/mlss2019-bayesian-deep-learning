{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLSS2019 -- Bayesian active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn what basic building blocks are needed\n",
    "to endow (deep) neural networks with uncertainty estimates, and how\n",
    "this can be used in active learning of a expert-in-the-loop pipelines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Linear, Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0x7fff_ffff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(722_257_201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.r_[:120].reshape(2, 3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.transpose(3, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some service functions\n",
    "This function converts numpy arrays into torch tensors and places them on the specified compute device.\n",
    "\n",
    "A procedure to plot a $1$-d function (to keep aesthetics in one place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import dataset_from_numpy\n",
    "\n",
    "from utils import plot1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty estimation in Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a function\n",
    "$$\n",
    "    f\n",
    "    \\colon \\mathbb{R} \\to \\mathbb{R}\n",
    "    \\colon x \\mapsto x \\sin x\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(start, stop, num=50, noise=0.01):\n",
    "    \"\"\"Simple toy 1-D function taken from Yarin's post_.\n",
    "\n",
    "    .. _post: http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html\n",
    "    \"\"\"\n",
    "    X = np.linspace(start, stop, num)\n",
    "    y = X * np.sin(X)\n",
    "\n",
    "    if noise > 0:\n",
    "        y += noise * random_state.randn(num)\n",
    "\n",
    "    return X[:, np.newaxis], y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the initial small dataset $S_0 = (x_i, y_i)_{i=1}^{m_0}$ of $m_0 = 20$\n",
    "samples with $y_i = f(x_i)$ and $x_i$ on a regular-spaced grid over $[-4, +4]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = map(np.concatenate, zip(\n",
    "    f(-6.0, +6.0, num=20, noise=0.0)\n",
    "))\n",
    "\n",
    "X_test, y_test = f(-10., +10., num=251, noise=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sample and $S_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1d(\"Train-test 1d function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    Linear(1, 512, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(512, affine=False),\n",
    "\n",
    "    Linear(512, 512, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(512, affine=False),\n",
    "\n",
    "    Linear(512, 1, bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model to device and convert numpy arrays to tensors (both train and test datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "train = dataset_from_numpy(X_train, y_train, device=device, dtype=torch.float)\n",
    "\n",
    "test = dataset_from_numpy(X_test, y_test, device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple fit loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataset, batch_size=32, n_epochs=1,\n",
    "        loss_fn=F.nll_loss, verbose=False):\n",
    "    \"\"\"Fit the model with SGD on the specified dataset.\"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    # an optimizer for model's parameters\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)  # , weight_decay=1e-5)\n",
    "\n",
    "    # minibatch generator for the training loop\n",
    "    feed = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in tqdm.tqdm(range(n_epochs), disable=not verbose):\n",
    "\n",
    "        model.train()\n",
    "        for X, y in feed:\n",
    "            # forward pass\n",
    "            output = model(X.to(device))\n",
    "\n",
    "            # criterion: batch-average loss\n",
    "            loss = loss_fn(output, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            # get gradients with backward pass\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # SGD update\n",
    "            optim.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model, train, n_epochs=2000, loss_fn=F.mse_loss, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(model, feed):\n",
    "    \"\"\"Collect model's outputs on the dataset without autograd.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # disable gradients (huge speed up!)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # compute and collect the outputs\n",
    "        return torch.cat([\n",
    "            model(X.to(device)).cpu() for X, *rest in feed\n",
    "        ], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feed = DataLoader(test, batch_size=512, shuffle=False)\n",
    "\n",
    "y_pred = apply(model, test_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the model predictions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1d(\"prediction\", y_pred.numpy()[np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that this model fits the curve adequately well.\n",
    "\n",
    "Can we somehow add uncertainty estimation to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian version of any model treats weights not as fixed parameters\n",
    "to be optimized over, but rather as random variables the distribution\n",
    "of which **after** observing the data is to be sought.\n",
    "\n",
    "* a prior distribution $p(\\omega)$ on the parameters $\\omega$ -- this\n",
    "  reflects our belief prior to observing data.\n",
    "\n",
    "* $p(y \\mid x, \\omega)$ -- the output distribution (Gaussian, Categorical,\n",
    "  Poisson), the parameters of which are modeled by some function of the\n",
    "  input $f_\\omega\\colon \\mathcal{X} \\to \\mathcal{Y}$. We construct the\n",
    "  **likelihood** with this building block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually assume that given the parameters $\\omega$ (and latent factors,\n",
    "e.g. source components in mixture models), that the targets are conditionally\n",
    "independent: $\n",
    "    p(D \\mid \\omega)\n",
    "        = \\prod_i p(y_i, x_i\\mid \\omega)\n",
    "$.\n",
    "\n",
    "From these blocks under these assumptions we wish to find $\n",
    "p(\\omega\\mid D) = \\tfrac{p(D\\mid \\omega) p(\\omega)}{p(D)}\n",
    "$ -- the **posterior parameter distribution**, and having marginalized\n",
    "$\\omega$ the **posterior predictive distribution** $p(y\\mid x, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless the distributions and likelihoods are conjugate, posterior in\n",
    "Bayesian inference is typically intractable: it is common to resort\n",
    "to **Variational Inference** or **Monte Carlo** approximations, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify a parametric family $Q$ of densities over the latent variables,\n",
    "i.e. the parameters of the network. Each $q(\\omega) \\in Q$ is a candidate approximation\n",
    "to the exact intractable posterior $p(\\omega \\mid D)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to measure difference between $q^*_D(z)$ and $p(z\\mid x)$\n",
    "(and its gradient) using only cheap operations. By assumption,\n",
    "we canâ€™t sample from $p(z\\mid x)$ or evaluate its density.\n",
    "\n",
    "We can\n",
    "* evaluate density $p(x, z)$ aka unnormalized $p(z\\mid x)$\n",
    "* sample from $q^*_D(z)$ and evaluate its density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the best candidate that approximates conditional density\n",
    "of latent variables given observed variables. One notion of **proximity**\n",
    "between distributions is the KL divergence. The Kullback-Leibler divergence\n",
    "between $P$ and $Q$ with densities $p$ and $q$, respectively, is given by\n",
    "\n",
    "$$\n",
    "    \\mathrm{KL}(q(\\omega) \\| p(\\omega))\n",
    "%         = \\mathbb{E}_{\\omega \\sim Q} \\log \\tfrac{dQ}{dP}(\\omega)\n",
    "        = \\mathbb{E}_{\\omega \\sim q(\\omega)}\n",
    "            \\log \\tfrac{q(\\omega)}{p(\\omega)}\n",
    "    \\,. $$\n",
    "\n",
    "Can get unbiased estimate using only samples from $q(z\\mid x)$ and evaluations\n",
    "of $q(z\\mid x)$ and $p(z, x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any distribution $q$ (density or probability mass function) and\n",
    "any likelihood of the data $p(D)$ we have\n",
    "\n",
    "$$\n",
    "    \\log p(D)\n",
    "%         = \\mathbb{E}_{\\omega \\sim q} \\log \\tfrac{p(\\omega, D)}{p(\\omega \\mid D)}\n",
    "%         = \\mathbb{E}_{\\omega \\sim q} \\log \\tfrac{p(\\omega, D)}{q(\\omega)}\n",
    "%         + \\mathbb{E}_{\\omega \\sim q} \\log \\tfrac{q(\\omega)}{p(\\omega \\mid D)}\n",
    "%         = \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega)\n",
    "%         + \\mathbb{E}_{\\omega \\sim q} \\log \\tfrac{p(\\omega)}{q(\\omega)}\n",
    "%         + \\mathbb{E}_{\\omega \\sim q} \\log \\tfrac{q(\\omega)}{p(\\omega \\mid D)}\n",
    "        = \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega)\n",
    "        - \\mathrm{KL}(q(\\omega)\\| p(\\omega))\n",
    "        + \\mathrm{KL}(q(\\omega)\\| p(\\omega \\mid D))\n",
    "    \\,, $$\n",
    "\n",
    "provided $q(\\omega) \\ll p(\\omega \\mid D)$.\n",
    "\n",
    "Therefore, we can optimize an alternative objective that is equivalent to the\n",
    "KL up to an added constant:\n",
    "\n",
    "$$\n",
    "    q^* \\in\n",
    "    \\arg\\max_{q\\in Q}\n",
    "        \\mathrm{ELBO}(q) = \n",
    "            \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega)\n",
    "            - \\mathrm{KL}(q(\\omega)\\| p(\\omega))\n",
    "    \\,. $$\n",
    "\n",
    "* the expected likelihood -- favours $q$ that place their mass on\n",
    "parameters $\\omega$ that explain the observed data under the specified\n",
    "model.\n",
    "\n",
    "* the negative KL-divergence -- encourages variational densities\n",
    "not to stray away too far from to the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the goal of **VI** in our case is to find\n",
    "\n",
    "$$\n",
    "    q^* \\in \\arg\\min_{q\\in Q} \\mathrm{KL}(q(\\omega)\\| p(\\omega \\mid D))\n",
    "    \\,. $$\n",
    "\n",
    "Although computing the divergence w.r.t. the posterior is still hard and\n",
    "intractable, it is possible due to the following identity based on the\n",
    "Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We consider the mean-field variational family of densities $Q$, where the parameters of the model are assumed to be mutually independent random variables each governed by a distinct factor in the variational density:\n",
    "\n",
    "$$\n",
    "    q(\\omega)\n",
    "        = \\prod_j q_j(\\omega_j)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a well-written review of variational inference see\n",
    "\n",
    "> Blei, D. M., Kucukelbir, A., & McAuliffe, J. D. (2017). Variational inference: A review for statisticians. [Journal of the American Statistical Association, 112(518), 859-877](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773).\n",
    "\n",
    "[arXiv:1601.00670](https://arxiv.org/pdf/1601.00670.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we clearly need to give our neural networks stochastic\n",
    "parameters and `pytorch` does not allow this out-of-the-box,\n",
    "we will have to build it ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall make:\n",
    "* a base class that tags derived classes as being capable of sampling their\n",
    "parameters $\\omega_j$ from $q_\\theta(\\omega_j)$\n",
    "* a procedure that crawls over the model's components, requests random draws\n",
    "  and collects parameter samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalModule(torch.nn.Module):\n",
    "    def sample(self):\n",
    "        raise NotImplementedError(\"Derived classes must implement a parameter \"\n",
    "                                  \"sampler from their own distribution.\")\n",
    "\n",
    "\n",
    "def named_variational_modules(module, prefix=\"\"):\n",
    "    for name, mod in module.named_modules():\n",
    "        if isinstance(mod, VariationalModule):\n",
    "            yield name, mod\n",
    "\n",
    "\n",
    "def named_parameter_samples(module, prefix=\"\"):\n",
    "    \"\"\"Returns an iterator over parameter draw from all stochastic\n",
    "    modules in the network, yielding both the name of the parameter\n",
    "    as well as the parameter itself.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    (string, torch.Tensor):\n",
    "        Tuple containing the name and sampled parameter\n",
    "    \"\"\"\n",
    "\n",
    "    for name, mod in named_variational_modules(module, prefix):\n",
    "\n",
    "        par_prefix = name + ('.' if name else '')\n",
    "        for key, par in mod.sample().items():\n",
    "            yield par_prefix + key, par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (implicit) Variational approximation via Bernoulli Dropout\n",
    "\n",
    "We can use the classical Bernoulli dropout, [Hinton et al. 2012](https://arxiv.org/abs/1207.0580)\n",
    "with as a variational approximation [Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf).\n",
    "\n",
    "```\n",
    "A simple regularization method allows uncertainty estimation essentially for free!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense linear layer\n",
    "\n",
    "The *classical dropout* acts upon the inputs into a linear layer.\n",
    "\n",
    "For input\n",
    "$\n",
    "    x\\in \\mathbb{R}^{[\\mathrm{in}]}\n",
    "$ and a layer parameters $\n",
    "    W\\in \\mathbb{R}^{[\\mathrm{out}] \\times [\\mathrm{in}]}\n",
    "$\n",
    "and $\n",
    "    b\\in \\mathbb{R}^{[\\mathrm{out}]}\n",
    "$ the resulting effect is\n",
    "\n",
    "$$\n",
    "    \\tilde{x} = x \\odot m\n",
    "    \\,, \\\\\n",
    "    y = \\tilde{x} W^\\top + b\n",
    "%     = b + \\sum_i x_i m_i W_i\n",
    "    \\,, $$\n",
    "\n",
    "where $\\odot$ is the elementwise product and $m\\in \\mathbb{R}^{[\\mathrm{in}]}$\n",
    "with $m_j \\sim \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$,\n",
    "i.e. equals $\\tfrac1{1-p}$ with probability $1-p$ and $0$ otherwise.\n",
    "\n",
    "Other variants of Bernoulli dropout have been studied as well (like `dropConnect`\n",
    "which zeroes out individual elements of the weight matrix), see\n",
    "[Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this fused operation. `torch` has\n",
    "* `F.dropout` for Bernouli dropout\n",
    "* `F.linear` for $y = x W^\\top + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_linear(layer, input, p=0.5):\n",
    "    \"\"\"Apply dropout and then affine transformation.\"\"\"\n",
    "\n",
    "    ## Exercise\n",
    "    input = F.dropout(input, p, True)\n",
    "    output = F.linear(input, layer.weight, layer.bias)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiplicative effect of the random mask $m$ on $x$ can be\n",
    "equivalently seen as a random **on/off** switch effect on the\n",
    "**columns** of the matrix $W$:\n",
    "\n",
    "$$\n",
    "    y_j\n",
    "%         = b_j + \\sum_i \\tilde{x}_i W_{ji}\n",
    "%         = b_j + \\sum_i x_i m_i W_{ji}\n",
    "        = b_j + \\sum_i x_i \\breve{W}_{ji}\n",
    "    \\,, $$\n",
    "\n",
    "where the each column of $\\breve{W}_i$ is, independently, either\n",
    "$\\mathbf{0}\\in \\mathbb{R}^{[\\mathrm{out}]}$ with probability $p$ or\n",
    "some (learnable) vector in $\\mathbb{R}^{[\\mathrm{out}]}$\n",
    "\n",
    "$$\n",
    "    \\breve{W}_i \\sim\n",
    "\\begin{cases}\n",
    "    \\mathbf{0}\n",
    "        & \\text{ w. prob } p \\,, \\\\\n",
    "    \\tfrac1{1-p} M_i\n",
    "        & \\text{ w. prob } 1-p \\,.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NOTE)** `pytorch` stores the matrix of the linear layer as $\n",
    "W \\in \\mathbb{R}^{\n",
    "    [\\mathrm{out}]\n",
    "    \\times [\\mathrm{in}]\n",
    "}\n",
    "$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_columns(weight, p=0.5):\n",
    "    \"\"\"Apply dropout with rate `p` to columns of `weight`.\"\"\"\n",
    "\n",
    "    ## Exercise\n",
    "    p = torch.full_like(weight[:1, :], 1 - p)\n",
    "    sample = weight * torch.bernoulli(p) / p\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the stochastic `Linear+Dropout` layer should:\n",
    "\n",
    "1. dropout the inputs and apply the linear (affine) transformation on forward pass\n",
    "2. randomly zero entire columns of the weight matrix $W$, when queried for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBernoulli(Linear, VariationalModule):\n",
    "    \"\"\"Linear layer with dropout on inputs.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):        \n",
    "        \"\"\"Apply dropout and then affine transformation.\"\"\"\n",
    "\n",
    "        return stochastic_linear(self, input, self.p)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Sample the weight from the variational distribution.\"\"\"\n",
    "\n",
    "        weight = dropout_columns(self.weight, self.p)\n",
    "        return {\"weight\": weight, \"bias\": self.bias}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)**\n",
    "This effect amounts to a `special` variational approximation $q_{\\theta}(W)$:\n",
    "\n",
    "$$\n",
    "    q_\\theta(W)\n",
    "        = \\prod_i q_\\theta(W_i)\n",
    "        = \\prod_i \\{\n",
    "            p \\delta_{\\mathbf{0}}(W_i)\n",
    "            + (1 - p) \\delta_{\\tfrac1{1-p} M_i}(W_i)\n",
    "        \\}\n",
    "    \\,, $$\n",
    "\n",
    "where $\\delta_\\mathbf{a}$ is a **point-mass** distribution at $\\mathbf{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating predictive uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches to estimating predictive uncertainty\n",
    "for the sample $\\tilde{S} = (\\tilde{x}_i)_{i=1}^m \\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-estimate approach ([blog: Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)):\n",
    "\n",
    "* for $i = 1... m$ do:\n",
    "\n",
    "  1. draw an iid sample of parameters $\\Omega = (\\omega_b)_{b=1}^B \\sim q_\\theta(\\omega)$\n",
    "  2. compute $y_{bi} = f_{\\omega_b}(\\tilde{x}_i)$ for $b=1 .. B$.\n",
    "\n",
    "\n",
    "* compute mean and variance of $\\hat{y}_{bi}$ along $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_estimate(model, dataset, n_samples=1, verbose=False):\n",
    "    \"\"\"Draw pointwise samples with stochastic forward pass.\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "    for _ in tqdm.tqdm(range(n_samples), disable=not verbose):\n",
    "\n",
    "        outputs.append(apply(model, dataset))\n",
    "\n",
    "    return torch.stack(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* uses stochastic forward passes -- no need to for extra code and classes\n",
    "* predictive distributions at adjacent inputs are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample function approach ([blog: Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html)):\n",
    "\n",
    "* for $b = 1... B$ do:\n",
    "\n",
    "  1. draw a realization $f_b\\colon \\mathcal{X} \\to \\mathcal{Y}$\n",
    "  with from the process $\\{f_\\omega\\}_{\\omega \\sim q_\\theta(\\omega)}$\n",
    "  2. get $\\hat{y}_{bi} = f_b(\\tilde{x}_i)$ for $i=1 .. m$\n",
    "\n",
    "\n",
    "* compute mean and variance of $\\hat{y}_{bi}$ along $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation performs the discussed steps verbatim:\n",
    "1. sample $f \\sim f_{\\omega \\sim q_\\theta(\\omega)}$ independently\n",
    "2. compute the outputs for each sample realization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_function(model, dataset, n_samples=1, verbose=False):\n",
    "    \"\"\"Draw a realization of a random function.\"\"\"\n",
    "\n",
    "    outputs = []\n",
    "    for _ in tqdm.tqdm(range(n_samples), disable=not verbose):\n",
    "        det_model = realization(model)\n",
    "\n",
    "        outputs.append(apply(det_model, dataset))\n",
    "\n",
    "    return torch.stack(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** we have not yet implemented the `realization(...)` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making our model Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to sample a realization of our Bayesian model\n",
    "regarded as a random function. For this the following steps seem \n",
    "sufficient:\n",
    "\n",
    "1. draw random parameters ($\\omega \\sim q_\\theta(\\omega)$)\n",
    "2. create a deterministic clone of the network $\\bar{f}_\\cdot$\n",
    "3. set $\\bar{f}_\\cdot$'s parameters to $\\omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realization(model):\n",
    "    parameters = model.state_dict()\n",
    "    parameters.update(named_parameter_samples(model))\n",
    "\n",
    "    det_model = model.deterministic()\n",
    "    det_model.to(next(model.parameters()).device)\n",
    "\n",
    "    det_model.load_state_dict(parameters, strict=False)\n",
    "\n",
    "    return det_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In light of the functionality implemented above and discussed \n",
    "in the reminder on uncertainty, our model object must be capable\n",
    "of:\n",
    "* performing a (stochastic) forward pass\n",
    "* producing a deterministic clone of itself on-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, l_linear=LinearBernoulli):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.body = torch.nn.Sequential(\n",
    "            Linear(1, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(512, affine=False),\n",
    "\n",
    "            l_linear(512, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(512, affine=False),\n",
    "\n",
    "            l_linear(512, 1, bias=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.body(input)\n",
    "\n",
    "    def deterministic(self):\n",
    "        \"\"\"Returns a deterministic version of self.\"\"\"\n",
    "        return type(self)(l_linear=Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new instance and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "fit(model, train, n_epochs=2000, loss_fn=F.mse_loss, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean and variance of the predictive distribution using Monte Carlo:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{y\\sim p(y\\mid x, D)} \\, g(y)\n",
    "        &\\overset{\\text{BI}}{=}\n",
    "            \\mathbb{E}_{\\omega\\sim p(\\omega\\mid D)}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega)} \\, g(y) \n",
    "        \\\\\n",
    "        &\\overset{\\text{VI}}{\\approx}\n",
    "            \\mathbb{E}_{\\omega\\sim q_\\theta(\\omega)}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega)} \\, g(y)\n",
    "        \\\\\n",
    "        &\\overset{\\text{MC}}{\\approx}\n",
    "%             \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "%                 \\mathbb{E}_{y\\sim p(y\\mid x, \\omega)} \\, g(y)\n",
    "            \\frac1{\\lvert \\mathcal{W}\\rvert} \\sum_{\\omega \\in \\mathcal{W}}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega)} \\, g(y)\n",
    "    \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W} = (\\omega_b)_{b=1}^B \\sim q_\\theta(\\omega)$\n",
    "-- iid samples from the variational approximation.\n",
    "\n",
    "**(NB)** recall that we assume $p(y \\mid x, \\omega)$ to be tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_pe = point_estimate(model, test_feed, n_samples=101, verbose=True)\n",
    "\n",
    "mean_pe, std_pe = outputs_pe.mean(dim=0), outputs_pe.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs_sf = sample_function(model, test_feed, n_samples=101, verbose=True)\n",
    "\n",
    "mean_sf, std_sf = outputs_sf.mean(dim=0), outputs_sf.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1d(\"Sample function\", outputs_sf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare point estimates with function sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = canvas1d()\n",
    "\n",
    "ax.plot(X_test[:, 0], outputs_sf[..., 0].numpy().T, c=\"C0\", alpha=0.05)\n",
    "ax.plot(X_test, mean_sf.numpy(), c=\"fuchsia\", lw=3, label=\"mean\")\n",
    "\n",
    "ax.plot(X_test, y_test, lw=2, color=\"k\", alpha=0.5, label=\"test\")\n",
    "ax.scatter(X_train, y_train, c=\"k\", s=20, label=\"train\")\n",
    "\n",
    "ax.set_title(\"Sample functions\")\n",
    "plt.legend(ncol=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = canvas1d()\n",
    "\n",
    "ax.plot(X_test[:, 0], outputs_pe[..., 0].numpy().T, c=\"C0\", alpha=0.05)\n",
    "ax.plot(X_test, mean_pe.numpy(), c=\"fuchsia\", lw=3, label=\"mean\")\n",
    "\n",
    "ax.plot(X_test, y_test, lw=2, color=\"k\", alpha=0.5, label=\"test\")\n",
    "ax.scatter(X_train, y_train, c=\"k\", s=20, label=\"train\")\n",
    "\n",
    "ax.set_title(\"Point estimates\")\n",
    "plt.legend(ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data labelling is costly and time consuming\n",
    "* unlabeled instances are essentially free\n",
    "\n",
    "**Goal** Achieve high performance with fewer labels by\n",
    "identifying the best instances to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential blocks of active learning:\n",
    "\n",
    "* a model $m$ or a function $a$ capable of quantifying uncertainty\n",
    "* a labelling **oracle**, e.g. a human expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential steps of active learning:\n",
    "\n",
    "1. fit $m$ on $\\mathcal{S}_{\\mathrm{labelled}}$\n",
    "2. get exact or approximate $\n",
    "    \\mathcal{S}^* \\in \\arg \\max_{U \\subseteq \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "        \\mathbf{a}(U; m)\n",
    "$ **without** access to targets and satisfying **budget constraints**\n",
    "3. request an **oracle** to provide labels for $\\mathcal{S}^*$\n",
    "4. update $\n",
    "\\mathcal{S}_{\\mathrm{labelled}}\n",
    "    \\leftarrow \\mathcal{S}^*\n",
    "        \\cup \\mathcal{S}_{\\mathrm{labelled}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many acquisition criteria (borrowed from [Gal17a](http://proceedings.mlr.press/v70/gal17a.html)):\n",
    "* Max entropy (plain uncertainty)\n",
    "* Maximal information about parameters and predictions (mutual information)\n",
    "* Variance ratios\n",
    "* Mean standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _entropy_ (all densities and/or probability mass functions can be conditional):\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p} \\log p(y)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BALD** (Bayesian Active Learning by Disagreement) acquisition criterion is the posterior\n",
    "mutual information between model's predictions at some point $x$ and model's parameters:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}(y\\,; \\omega \\mid x, D_\\mathrm{train})\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim p(\\omega\\mid D_\\mathrm{train})}\n",
    "                p(y\\mid x, \\omega, D_\\mathrm{train})\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim p(\\omega\\mid D_\\mathrm{train})}\n",
    "            \\mathbb{H}(p(y\\mid x, \\omega, D_\\mathrm{train}))\n",
    "    \\,. \\tag{mi-bi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational Inference approximation of **BALD** is given by\n",
    "the same expression, except with $q_\\theta(\\omega)$ instead of\n",
    "$p(\\omega\\mid D_\\mathrm{train})$:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{vi}(y\\,; \\omega \\mid x, D_\\mathrm{train})\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "                p(y\\mid x, \\omega, D_\\mathrm{train})\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "            \\mathbb{H}(p(y\\mid x, \\omega, D_\\mathrm{train}))\n",
    "    \\,. \\tag{mi-vi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an iid sample $\\mathcal{W} = (\\omega_b)_{b=1}^B \\sim q_\\theta(\\omega)$\n",
    "of size $B$. The Monte Carlo approximation of the mutual information is\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{MC}(y\\,; \\omega \\mid x, D_\\mathrm{train})\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "                p(y\\mid x, \\omega, D_\\mathrm{train})\n",
    "        \\bigr)\n",
    "        - \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "            \\mathbb{H}(p(y\\mid x, \\omega, D_\\mathrm{train}))\n",
    "    \\,, \\tag{mi-mc} $$\n",
    "\n",
    "where $\\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}} h(\\omega) = \\tfrac1B \\sum_j h(\\omega_j)$\n",
    "denoted the expectation with respect to the empirical probability measure induced\n",
    "by the sample $\\mathcal{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block: entropy and mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete distributions $p(y) \\in \\delta_m$ we can use KL-divergence\n",
    "to compute entropy:\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = \\log n - \\mathbb{E}_{y\\sim p} \\log \\tfrac{p_y}{\\tfrac1n}\n",
    "        = - KL(p\\|\\tfrac{\\mathbf{1}}n)\n",
    "    \\,, $$\n",
    "basically, the Kullback-Leibler divergence of $p(y)$ from a uniformly random choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)** We do this, because pytorch has a **numerically stable** KL-div implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(proba):\n",
    "\n",
    "    # return - torch.sum(proba * torch.log(proba), dim=-1)\n",
    "\n",
    "    return - torch.kl_div(torch.zeros_like(proba), proba).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a procedure that computes relevant estimates of the posterior\n",
    "predictve distribution, namely\n",
    "* its sample approximation $\n",
    "\\hat{p}(y\\mid x, D) = \\hat{\\mathbb{E}}_{\\omega \\sim\\mathcal{W}} p(y \\mid x, \\omega)\n",
    "$\n",
    "\n",
    "* its sample entropy $\\hat{H}(y\\mid x, D) = \\mathbb{H}\\bigl(\\hat{p}(y\\mid x, D)\\bigr)$\n",
    "\n",
    "* the _mutual information_ $\\hat{I}(y; \\omega\\mid x, D)$:\n",
    "$$\n",
    "    \\mathbb{I}(y; \\omega)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega} p(y\\mid \\omega)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega}\n",
    "            \\mathbb{H}(p(y\\mid \\omega))\n",
    "    \\,. $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(proba):\n",
    "    # average across function samples\n",
    "    avg_proba = proba.mean(dim=0)\n",
    "\n",
    "    # mutual information components for `H E_w p(., w) - E_w H p(., w)`\n",
    "    ent_exp = entropy(avg_proba)\n",
    "    exp_ent = entropy(proba).mean(dim=0)\n",
    "\n",
    "    mut_info = ent_exp - exp_ent\n",
    "\n",
    "    return avg_proba, ent_exp, mut_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block: prediction, evaluation and acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, n_samples=1, kind=\"function\"):\n",
    "    feed = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    # Monte-Carlo function samples\n",
    "    if kind == \"function\":\n",
    "        logits = sample_function(model, feed, n_samples=n_samples)\n",
    "\n",
    "    else:\n",
    "        logits = point_estimate(model, feed, n_samples=n_samples)\n",
    "\n",
    "    # logit-scores should be transformed to a proper distribution\n",
    "    proba = F.softmax(logits, dim=-1)\n",
    "\n",
    "    return mutual_information(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)** when working with floating point numbers always try to pay\n",
    "attention to numeric stability issues:\n",
    "\n",
    "$$\n",
    "\\mathop{softmax}{z}\n",
    "    = \\bigl( \\tfrac{e^{z_i}}{\\sum_j e^{z_j} } \\bigr)_{i=1}^n\n",
    "    \\,, $$\n",
    "here you would likely use the *log-sum-exp* trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the measure of uncertainty for classification we can use entropy of the predictive distribution.\n",
    "\n",
    "For regression, as we have seen in the $1d$ examples above, we used the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, n_samples=1):\n",
    "    proba, entropy, mutual_info = predict(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    predicted = proba.numpy().argmax(axis=-1)\n",
    "    target = dataset.tensors[1].cpu().numpy()\n",
    "\n",
    "    return confusion_matrix(target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BALD** acquisition function works like this:\n",
    "* estimate mutual information $\\mathbb{I}_\\mathrm{vi}(y \\,; \\omega \\mid x, D_\\mathrm{train})$\n",
    "  for each instance $x$ in $\\mathcal{S}_\\mathrm{pool}$\n",
    "* pick the top $\\ell$ instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gal, Y., Islam, R. & Ghahramani, Z.. (2017). Deep Bayesian Active Learning with Image Data. Proceedings of the 34th International Conference on Machine Learning, in [PMLR 70:1183-1192](http://proceedings.mlr.press/v70/gal17a.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquisition(model, dataset, n_points=10, n_samples=1):\n",
    "    proba, entropy, mutual_info = predict(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    indices = mutual_info.argsort()\n",
    "\n",
    "    return indices[-n_points:], proba, entropy, mutual_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)** A drawback of the `pointwise` top-$\\ell$ procedure above is\n",
    "that although it acquires individually informative instances, altogether\n",
    "they might end up **being** `jointly poorly informative`.\n",
    "\n",
    "This can be corrected if we would seek the highest mutual information\n",
    "among sets $\\mathcal{S} = (x_i)_{i=1}^\\ell$. This leads to combinatorial\n",
    "explosion of the amount of computations and memory requirements, however\n",
    "there are working solutions like random sampling of subsets $\\mathcal{S}$\n",
    "of size $\\ell$ from $\\mathcal{S}_\\mathrm{pool}$ or greedy maximization\n",
    "of this *submodular* criterion.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{I}_\\mathrm{vi}(\\{y_1, ..., y_\\ell\\}\\,; \\omega \\mid \\{x_1, ..., x_\\ell\\}, D_\\mathrm{train})\n",
    "        &= \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "                p(\\{y_1, ..., y_\\ell\\}\\mid \\{x_1, ..., x_\\ell\\}, \\omega, D_\\mathrm{train})\n",
    "        \\bigr)\n",
    "        \\\\\n",
    "        % conditional independence given \\omega\n",
    "        % &- \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "        %     \\mathbb{H}(p(\\{y_1, ..., y_\\ell\\}\\mid \\{x_1, ..., x_\\ell\\}, \\omega, D_\\mathrm{train}))\n",
    "        &- \\sum_{i=1}^\\ell \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "            \\mathbb{H}(p(y_i\\mid x_i, \\omega, D_\\mathrm{train}))\n",
    "\\end{align}\n",
    "        \\,. \\tag{mi-vi-batch} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kirsch, A., van Amersfoort, J., & Gal, Y. (2019). BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. arXiv preprint [arXiv:1906.08158](https://arxiv.org/abs/1906.08158.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active Learning with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will partially replicate figure 1. in [Gat et al. (2017): p. 4](http://proceedings.mlr.press/v70/gal17a.html),\n",
    "\n",
    "> Gal, Y., Islam, R. & Ghahramani, Z.. (2017). Deep Bayesian Active Learning with Image Data. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:1183-1192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the datasets from the `train` part of [MNIST](http://yann.lecun.com/exdb/mnist/):\n",
    "* ($\\mathcal{S}_\\mathrm{train}$) initial **training**:\n",
    "  $\\approx 20$ images, purposefully highly imbalanced classes (even absent classes)\n",
    "* ($\\mathcal{S}_\\mathrm{valid}$) our **validation**:\n",
    "  $5000$ images, stratified\n",
    "* ($\\mathcal{S}_\\mathrm{pool}$) acquisition **pool**:\n",
    "  all remaining images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(\"\"\"./data\"\"\",\n",
    "                         train=True, download=True, transform=transform)\n",
    "\n",
    "test = datasets.MNIST(\"\"\"./data\"\"\",\n",
    "                      train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)** We may just as well use [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an imbalanced class label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 20\n",
    "\n",
    "distribution = random_state.dirichlet([0.1] * 10)\n",
    "initial_imbalance = np.round(distribution * n_train).astype(int)\n",
    "\n",
    "initial_imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get indices for each subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset.targets.numpy()\n",
    "\n",
    "ix_rest, ix_valid = train_test_split(\n",
    "    np.arange(len(targets)), stratify=targets, test_size=5000,\n",
    "    shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the specified number of instances from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for label, freq in enumerate(initial_imbalance):\n",
    "    ix = np.flatnonzero(targets[ix_rest] == label)\n",
    "    indices.extend(ix[:freq])\n",
    "\n",
    "ix_train = np.take(ix_rest, indices)\n",
    "ix_pool = np.delete(ix_rest, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset:\n",
    "* The reason for the following procedure is that MNIST torchvision\n",
    "  dataset allows only single element indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images(dataset, indices):\n",
    "    \"\"\"MNIST torchvision dataset\"\"\"\n",
    "    pairs = (dataset[i] for i in tqdm.tqdm(indices))\n",
    "\n",
    "    data, target = zip(*pairs)\n",
    "\n",
    "    return torch.stack(data, dim=0), torch.tensor(target)\n",
    "\n",
    "S_train = TensorDataset(*collect_images(dataset, ix_train))\n",
    "S_valid = TensorDataset(*collect_images(dataset, ix_valid))\n",
    "S_pool  = TensorDataset(*collect_images(dataset, ix_pool ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from evaluating and the acquisition criteria, we will need to be\n",
    "able to manipulate the datasets for the **main active learning** loop.\n",
    "\n",
    "We begin by implementing the following primitives:\n",
    "* `take` collect the instances at the specified indices into a **new dataset** (object)\n",
    "* `append` add one dataset to another\n",
    "* `delete` drops the instances at the specified locations form the copy of the **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(pool, indices):\n",
    "    \"\"\"Copy the specified samples from the pool.\"\"\"\n",
    "\n",
    "    mask = torch.zeros(len(pool), dtype=torch.bool)\n",
    "    mask[indices] = True\n",
    "\n",
    "    return TensorDataset(*pool[mask])\n",
    "\n",
    "\n",
    "def delete(pool, indices):\n",
    "    \"\"\"Drop the specified samples from the pool.\"\"\"\n",
    "\n",
    "    mask = torch.ones(len(pool), dtype=torch.bool)\n",
    "    mask[indices] = False\n",
    "\n",
    "    return TensorDataset(*pool[mask])\n",
    "\n",
    "\n",
    "def append(train, new):\n",
    "    \"\"\"Append new samples to the train dataset.\"\"\"\n",
    "    tensors = [\n",
    "        torch.cat(pair, dim=0)\n",
    "        for pair in zip(train.tensors, new.tensors)\n",
    "    ]\n",
    "\n",
    "    return TensorDataset(*tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $2$-d Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, in convolutional neural networks the dropout acts upon the feature\n",
    "(channel) information and not on the spatial dimensions. Thus entire channels\n",
    "are dropped out and for $\n",
    "    x \\in \\mathbb{R}^{\n",
    "        [\\mathrm{in}]\n",
    "        \\times h\n",
    "        \\times w}\n",
    "$ and $\n",
    "    y \\in \\mathbb{R}^{\n",
    "        [\\mathrm{out}]\n",
    "        \\times h'\n",
    "        \\times w'}\n",
    "$ the full effect of the `Dropout+Conv2d` layer is\n",
    "\n",
    "$$\n",
    "    y_{lij} = ((x \\odot m) \\ast W_l)_{ij} + b_l\n",
    "        = b_l + \\sum_k \\sum_{pq} x_{k i_p j_q} m_k W_{lkpq}\n",
    "    \\,, \\tag{conv-2d} $$\n",
    "    \n",
    "where i.i.d $m_k \\sim \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$,\n",
    "and indices $i_p$ and $j_q$ represent the spatial location in $x$ that correspond\n",
    "to the $p$ and $q$ elements in the kernel $\n",
    "    W\\in \\mathbb{R}^{\n",
    "        [\\mathrm{out}]\n",
    "        \\times [\\mathrm{in}]\n",
    "        \\times h\n",
    "        \\times w}\n",
    "$ relative to $(i, j)$ coordinates in $y$.\n",
    "The exact values of $i_p$ and $j_q$ depend on the configuration of the\n",
    "convolutional layer, e.g. stride, kernel size and dilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBernoulli(torch.nn.Conv2d, VariationalModule):\n",
    "    \"\"\"Linear layer with dropout on inputs.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, p=0.05,\n",
    "                 padding_mode='zeros'):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                         padding=padding, dilation=dilation, groups=groups,\n",
    "                         bias=bias, padding_mode=padding_mode)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):        \n",
    "        \"\"\"Apply dropout and then the convolution.\"\"\"\n",
    "\n",
    "        # Exercise\n",
    "        return super().forward(F.dropout2d(input, self.p, True))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Sample the weight from the variational distribution.\"\"\"\n",
    "\n",
    "        # Exercise\n",
    "        p = torch.full_like(self.weight[:1, :, :1, :1], 1 - self.p)\n",
    "        weight = self.weight * torch.bernoulli(p) / p\n",
    "\n",
    "        return {\"weight\": weight, \"bias\": self.bias}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NB)** For more on convolutions see [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic) \n",
    "    repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the `SimpleModel` class above in $1-d$ study,\n",
    "let's implement a simple deep convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    \"\"\"A convolutional net.\"\"\"\n",
    "    def __init__(self, l_conv2d=Conv2dBernoulli, l_linear=LinearBernoulli):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = l_conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = l_linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = l_linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), 2, 2)\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), 2, 2)\n",
    "        x = F.relu(self.fc1(x.reshape(-1, 4 * 4 * 50)))\n",
    "        return F.log_softmax(self.fc2(x), dim=1)\n",
    "\n",
    "    def deterministic(self):\n",
    "        \"\"\"Return a deterministic version of self.\"\"\"\n",
    "        return type(self)(l_conv2d=Conv2d, l_linear=Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some service functions\n",
    "\n",
    "* The first computes the label frequencies\n",
    "* The second -- prepares a text representation of the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_counts(labels, n_labels=10):\n",
    "    return np.bincount(labels.numpy(), minlength=10)\n",
    "\n",
    "def display_counts(labels):\n",
    "    body = [f\"{n:2d}\" if n > 0 else \" .\" for n in label_counts(labels)]\n",
    "    return \"[ \" + ' '.join(body) + \" ]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Bayesian Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code the active learning loop. Recall that it\n",
    "consists of the following steps:\n",
    "\n",
    "1. fit on **train**\n",
    "2. evaluate on **holdout**\n",
    "3. acquire from **pool**\n",
    "4. add to **train**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs, n_samples = 20, 11\n",
    "n_active, n_points = 50, 10\n",
    "\n",
    "scores = []\n",
    "balances = [label_counts(S_train.tensors[1])]\n",
    "\n",
    "balance_str = display_counts(S_train.tensors[1])\n",
    "print(f\">>> # {len(S_train):4d}: (starting) {balance_str}\")\n",
    "\n",
    "for step in range(n_active):\n",
    "    # 1. fit\n",
    "    fit(model, S_train, n_epochs=n_epochs)\n",
    "\n",
    "    # 2. track validation score\n",
    "    score_matrix = evaluate(model, S_valid, n_samples=n_samples)\n",
    "\n",
    "    # 3. acquire new instances\n",
    "    indices, proba, ent, mutual_info = acquisition(\n",
    "        model, S_pool, n_points=n_points, n_samples=n_samples)\n",
    "\n",
    "    # 4. query the pool for the chosen instances\n",
    "    S_requested = take(S_pool, indices)\n",
    "    S_train = append(S_train, S_requested)\n",
    "    S_pool = delete(S_pool, indices)\n",
    "\n",
    "    # 5. (optional) report accuracy and the statistics on the acquired batch\n",
    "    tp = score_matrix.diagonal()\n",
    "    accuracy = tp.sum() / score_matrix.sum()\n",
    "\n",
    "    balance_str = display_counts(S_requested.tensors[1])\n",
    "    print(f\">>> # {len(S_train):4d}: (acquired) {balance_str}\"\n",
    "          f\" (Accuracy) {accuracy:.2%}\")\n",
    "\n",
    "    balances.append(label_counts(S_train.tensors[1]))\n",
    "    scores.append(score_matrix)\n",
    "\n",
    "\n",
    "fit(model, S_train, n_epochs=n_epochs)\n",
    "scores.append(evaluate(model, S_valid, n_samples=n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances = np.stack(balances, axis=0)\n",
    "scores = np.stack(scores, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the frequency of each class in $\\mathcal{S}_\\mathrm{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(balances);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of precision / recall scores on in $\\mathcal{S}_\\mathrm{valid}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = scores.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = scores.sum(axis=-2) - tp, scores.sum(axis=-1) - tp\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(tp / (tp + fp))\n",
    "ax[0].set_title(\"Precision (ovr)\")\n",
    "\n",
    "ax[1].plot(tp / (tp + fn))\n",
    "ax[1].set_title(\"Recall (ovr)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy as a function of active learning iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tp.sum(-1) / scores.sum((-2, -1)), label='Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume of data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"train : pool = {len(S_train)} : {len(S_pool)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the test set confusion matrix be the ultimate judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_test = TensorDataset(*collect_images(test, np.r_[:len(test)]))\n",
    "\n",
    "score_matrix = evaluate(model, S_test, n_samples=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives, and false positives / negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = score_matrix.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = score_matrix.sum(axis=-2) - tp, score_matrix.sum(axis=-1) - tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"(accuracy) {tp.sum() / score_matrix.sum():.2%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-v-rest precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fp))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovr recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fn))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mutual_info.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ent.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "groups = defaultdict(list)\n",
    "for l, e in zip(S_valid.tensors[1], ent):\n",
    "    groups[int(l)].append(float(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(groups.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersting stuff, that didn't make the cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense network for MNIST, in case convolutions are too slow on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Sequential):\n",
    "    \"\"\"A fully connected net.\"\"\"\n",
    "    def __init__(self, l_linear=LinearBernoulli):\n",
    "        super().__init__(\n",
    "            Linear(784, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            l_linear(256, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            l_linear(256, 256),\n",
    "            torch.nn.ReLU(),            \n",
    "            l_linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = super().forward(input.flatten(1))\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "    def deterministic(self):\n",
    "        \"\"\"Return a deterministic version of self.\"\"\"\n",
    "        return type(self)(l_linear=Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **any** $q(\\omega)$ (point-mass, products, mixtures, frank-wolfe\n",
    "boosted convex ensembles, even input dependent, **anything**) and any\n",
    "$\\phi$\n",
    "$$\n",
    "    \\overbrace{\n",
    "        \\log p(D; \\phi)\n",
    "    }^{\\text{unconditional likelihood}}\n",
    "        = \\underbrace{\n",
    "            \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega; \\phi)\n",
    "        }_{\\text{expected conditional likelihood}}\n",
    "        - \\overbrace{\n",
    "            \\mathrm{KL}(q(\\omega)\\| p(\\omega; \\phi))\n",
    "        }^{\\text{staying close to prior}}\n",
    "        + \\underbrace{\n",
    "            \\mathrm{KL}(q(\\omega)\\| p(\\omega \\mid D; \\phi))\n",
    "        }_{\\text{posterior approximation}}\n",
    "    \\,. \\tag{master-identity} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational approximation via Gaussian Mean Field Family\n",
    "\n",
    "* sampling from $q_\\theta(\\omega) = q_\\theta(b) \\otimes q_\\theta(W)$ where\n",
    "$q_\\theta(b) = \\delta_{b - \\beta}$, $q_\\theta(W) = \\otimes_{ij} q_\\theta(W_{ij})$\n",
    "and $\n",
    "q_\\theta(W_{ij})\n",
    "    = \\mathcal{N}\\bigl(\n",
    "        W_{ij} \\big\\vert\n",
    "            \\mu_{ij}, \\sigma^2_{ij}\n",
    "        \\bigr)\n",
    "$;\n",
    "* **local** reparameterization trick $y = x W + b$ implies that $\n",
    "    y_j \\sim \\mathcal{N}\\bigl(\n",
    "            \\beta_j + \\sum_i x_i \\mu_{ij},\n",
    "            \\sum_i \\sigma^2_{ij} \\lvert x_i \\rvert^2\n",
    "        \\bigr)\n",
    "$ and $y_j \\bot y_k$, $j\\neq k$;\n",
    "* computing the KL-divergence of $q_\\theta(W)$ from $p(W)$ ignoring $b$: $\n",
    "    \\mathop{KL}\\bigl(q_\\theta(W) \\| p(W)\\bigr)\n",
    "        = \\mathbb{E}_{W \\sim q_\\theta}\n",
    "            \\log \\tfrac{q_\\theta(W)}{p(W)}\n",
    "$\n",
    "  * diffuse $p(W_{ij}) \\propto \\mathop{const}$\n",
    "  * scale-free $p(W_{ij}) \\propto \\tfrac1{\\lvert W_{ij} \\rvert}$\n",
    "  * proper standard normal $p(W_{ij}) = \\mathcal{N}(W_{ij} \\mid 0, \\nu^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the this to work, it is necessary to implement a penalty \"collector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalties(module):\n",
    "    \"\"\"Returns an iterator over all penalties in the network.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    troch.Tensor:\n",
    "        Tuple value of the penalty.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    Penalties from shared modules are returned only once.\n",
    "    \"\"\"\n",
    "    for name, mod in named_variational_modules(module):\n",
    "        yield mod.penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all $\\phi$\n",
    "$$\n",
    "    \\log p(D; \\phi)\n",
    "    \\geq \\max_{\\theta}\n",
    "        \\mathrm{ELBO}(\\theta, \\phi)\n",
    "            = \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)} \\log p(D \\mid \\omega; \\phi)\n",
    "            - \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)} \\log \\tfrac{q_\\theta(\\omega)}{p_\\phi(\\omega)}\n",
    "    \\,, $$\n",
    "naturaly yields a coordinate-wise ascent algorithm:\n",
    "* **(E)** step $\\theta$ fixed $\\phi$\n",
    "* **(M)** step $\\phi$ fixed $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffuse prior $p(W_{ij}) \\propto \\mathop{const}$\n",
    "\n",
    "Against a **diffuse prior** the KL-divergence is just\n",
    "the **negative entropy** (up to a constant).\n",
    "\n",
    "Since $q_\\theta(W)$ is a diagonal multivariate normal\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_{ij} KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "        = \\mathop{const} - \\sum_{ij} \\mathbb{H}(q_\\theta(W_{ij}))\n",
    "    \\,, $$\n",
    "\n",
    "where $\\mathbb{H}(q)$ is the [(differential) entropy ](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multivariate Gaussain $\n",
    "    p(z) = \\mathcal{N}_n\\bigl(\n",
    "        z\\,\\big\\vert\\, \\mu, \\Sigma\n",
    "    \\bigr)\n",
    "$ we have\n",
    "$$\n",
    "    \\mathbb{H}(p)\n",
    "        = - \\mathbb{E}_{z\\sim p} \\log p(z)\n",
    "        = \\tfrac12 \\log \\det \\bigl(2 \\pi e \\Sigma \\bigr)\n",
    "%         = \\tfrac12 \\log \\det \\Sigma + \\tfrac{n}2 \\log 2 \\pi e\n",
    "    \\,. $$\n",
    "\n",
    "Hence the entropy for a univariate Gaussian is\n",
    "$$\n",
    "    \\mathbb{H}(q_\\theta(W_{ij}))\n",
    "        = - \\mathbb{E}_{W_{ij} \\sim q_\\theta(W_{ij})} \\log q_\\theta(W_{ij})\n",
    "        = \\tfrac12 \\log \\{2 \\pi e \\, \\sigma^2\\}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_diffuse(log_sigma2, weight=None):\n",
    "\n",
    "    const = 0.5 * math.log(2 * math.pi * math.e) * log_sigma2.numel()\n",
    "    entropy = const + 0.5 * torch.sum(log_sigma2)\n",
    "\n",
    "    return - entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper prior $p(W_{ij}) = \\mathcal{N}(W_{ij} \\mid 0, \\nu^{-1})$\n",
    "\n",
    "The KL-divergence between two multivariate Gaussians is given by\n",
    "\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\mathcal{N}_m(\\mu_0, \\Sigma_0)\n",
    "        \\big\\| \\mathcal{N}_m(\\mu_1, \\Sigma_1)\n",
    "    \\bigr)\n",
    "        = \\frac12 \\Bigl\\{\n",
    "            \\log \\frac{\\det \\Sigma_1}{\\det \\Sigma_0}\n",
    "            + \\mathop{tr} \\bigl( \\Sigma_1^{-1} \\Sigma_0 \\bigr)\n",
    "            + \\bigl(\\mu_0 - \\mu_1 \\bigr)^\\top \\Sigma_1^{-1} \\bigl(\\mu_0 - \\mu_1 \\bigr)\n",
    "            - m\n",
    "        \\Bigr\\}\n",
    "    \\,. $$\n",
    "\n",
    "KL divergence from a standard normal distribution is\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\mathcal{N}_m(\\mu_0, \\Sigma_0)\n",
    "        \\big\\| \\mathcal{N}_m(0, \\nu^{-1} I_m)\n",
    "    \\bigr)\n",
    "        = \\frac12 \\Bigl\\{\n",
    "            \\nu \\, (\\mathop{tr} \\Sigma_0 + \\mu_0^\\top \\mu_0)\n",
    "            - m \\log \\nu\n",
    "            - \\log \\det \\Sigma_0\n",
    "            - m\n",
    "        \\Bigr\\}\n",
    "    \\,. $$\n",
    "<!--\n",
    "$$\n",
    "    \\frac12 \\Bigl\\{\n",
    "        - \\log \\det \\nu \\Sigma_0\n",
    "        + \\nu \\mathop{tr} \\bigl( \\Sigma_0 \\bigr)\n",
    "        + \\nu \\bigl(\\mu_0 - \\mu_1 \\bigr)^\\top \\bigl(\\mu_0 - \\mu_1 \\bigr)\n",
    "        - m\n",
    "    \\Bigr\\}\n",
    "    \\,. $$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if $q_\\theta(W)$ is a diagonal multivariate normal then we get\n",
    "(put $m=1$, $\\Sigma_0 = \\sigma^2_{ij}$, $\\mu_1 = 0$):\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_{ij} KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "        = \\frac12 \\sum_{ij} \\bigl(\n",
    "            \\nu \\sigma^2_{ij} + \\nu \\mu_{ij}^2\n",
    "            - \\log \\sigma^2_{ij} - \\log \\nu - 1\n",
    "        \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_proper(log_sigma2, weight, nu=1.0):\n",
    "\n",
    "    const = 0.5 * (math.log(nu) + 1) * log_sigma2.numel()\n",
    "    nu_term = 0.5 * nu * (torch.exp(log_sigma2) + weight * weight)\n",
    "    kl_div = torch.sum(nu_term - log_sigma2) - const\n",
    "\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_1811_00596(log_sigma2, weight):\n",
    "    \"\"\"Penalty from arxiv:1811.00596.\"\"\"\n",
    "    # get $- \\log \\alpha_{ij}$\n",
    "    neg_log_alpha = 2 * torch.log(abs(weight) + 1e-12) - log_sigma2\n",
    "\n",
    "    # `softplus` is $x \\mapsto \\log(1 + e^x)$\n",
    "    kl_div_approx = torch.sum(0.5 * F.softplus(neg_log_alpha))\n",
    "\n",
    "    return kl_div_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale-free prior $p(W_{ij}) \\propto \\tfrac1{\\lvert W_{ij} \\rvert}$\n",
    "\n",
    "This prior gives us the so called Variational Dropout\n",
    "([Molchanov et al. 2017](https://arxiv.org/abs/1701.05369),\n",
    "[Kingma et al. 2015](https://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick)).\n",
    "\n",
    "We may observe the following, unless $\\mu_{ij} = 0$:\n",
    "for $\\alpha_{ij} = \\tfrac{\\sigma^2_{ij}}{\\mu_{ij}^2}$\n",
    "\n",
    "$$\n",
    "    \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij})\n",
    "    \\overset{D}{\\sim} \\mu_{ij} \\cdot \\mathcal{N}(1, \\alpha_{ij})\n",
    "    \\,. $$\n",
    "\n",
    "Therefore our variational approximation $q_\\theta(W)$ can be regarded as\n",
    "the so called Gaussian Dropout (): $W_{ij}$ are subject to multiplicative\n",
    "noise $W_{ij} = \\mu_{ij} \\cdot \\varepsilon_{ij}$ for $\\varepsilon_{ij}\n",
    "\\sim \\mathcal{N}(1, \\alpha_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this variational family:\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "%         = \\mathop{const} - \\tfrac12 \\log \\sigma^2_{ij}\n",
    "%         + \\mathbb{E}_{W_{ij} \\sim q_\\theta(W_{ij})} \\log \\lvert W_{ij} \\rvert\n",
    "        = \\mathop{const} - \\tfrac12 \\log \\sigma^2_{ij}\n",
    "        + \\tfrac12 \\log \\mu_{ij}^2\n",
    "        + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "            \\log \\lvert 1 + \\tfrac{\\sigma_{ij}}{\\lvert \\mu_{ij} \\rvert} \\xi \\rvert\n",
    "    \\,. $$\n",
    "\n",
    "Unfortunately there is no closed-from expression for this divergence, and thus\n",
    "we have to resort to its approximation in ICML'17 paper [Molchanov et al. 2017](https://arxiv.org/abs/1701.05369):\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "%         = \\mathop{const}\n",
    "%         + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(1, \\alpha_{ij})}\n",
    "%                         \\log{\\lvert \\xi \\rvert}\n",
    "%                     - \\tfrac12 \\log \\alpha_{ij}\n",
    "        \\approx\n",
    "            \\tfrac12 \\log (1 + e^{-\\log \\alpha}) \n",
    "            + k_1(1 - \\sigma(k_2 + k_3 \\log \\alpha))\n",
    "    \\,, $$\n",
    "\n",
    "where $k_1, k_2, k_3$ and $C$ are approximated by $0.63576$, $1.87320$ and $1.48695$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_scale_free(log_sigma2, weight):\n",
    "\n",
    "    # get $- \\log \\alpha_{ij}$\n",
    "    neg_log_alpha = 2 * torch.log(abs(weight) + 1e-12) - log_sigma2\n",
    "    \n",
    "    # Use the identity 1 - \\sigma(z) = \\sigma(- z)\n",
    "    sigmoid = torch.sigmoid(1.48695 * neg_log_alpha - 1.87320)\n",
    "\n",
    "    # `softplus` is $x \\mapsto \\log(1 + e^x)$\n",
    "    kl_div_approx_ij = 0.5 * F.softplus(neg_log_alpha) + 0.63576 * sigmoid\n",
    "    kl_div_approx = torch.sum(kl_div_approx_ij)\n",
    "\n",
    "    return kl_div_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The local reparameterization tirck\n",
    "\n",
    "Proposed in\n",
    "[Kingma et al. 2015](https://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick)\n",
    "this tirck\n",
    "* allows to sample parameters implicitly\n",
    "* reduces variance of the stochastic gradient, generally leading to much faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of a linear layer on its input is given by the equation\n",
    "$$\n",
    "    y = x W + b\n",
    "    \\,, \\tag{obvious} $$\n",
    "with $W \\in \\mathbb{R}^{m\\times n}$, $y\\in \\mathbb{R}^n$,\n",
    "$x\\in \\mathbb{R}^m$ and $b \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observation:\n",
    "> any non-trivial linear combination of Gaussian\n",
    "random variables is a Gaussian random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the layer's $W$ are jointly Gaussian, the means that the distribution\n",
    "of $y$ is also Gaussian. We can see this if we look closely at the output: the bias term is effectively\n",
    "fixed to $\\beta$ and the weights are\n",
    "\n",
    "$$\n",
    "    W_{ij} \\sim\n",
    "        \\mathcal{N}(M_{ij}, \\sigma^2_{ij})\n",
    "    \\,,\\, W_{ij} \\bot W_{kl}\n",
    "    \\,,\\, ij\\neq kl\n",
    "    \\,,\n",
    "$$\n",
    "\n",
    "which means that\n",
    "$$\n",
    "    y_j = \\beta_j + \\sum_i x_i W_{ij}\n",
    "    \\Rightarrow\n",
    "    y_j \\sim \\mathcal{N}\\bigl(\n",
    "        \\beta_j + \\sum_i x_i M_{ij}, \\sum_i \\sigma^2_{ij} x_i^2\n",
    "    \\bigr)\n",
    "    \\tag{local-reparametrization}\n",
    "    \\,. $$\n",
    "\n",
    "Note that independence of $W_{ij}$ implies zero correlation between them, which\n",
    "means that distinct $y_j$ and $y_k$ are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting into a single multivariate Gaussian random vector we get the \n",
    "following stochastic forward pass:\n",
    "\n",
    "$$\n",
    "    y \\sim \\mathcal{N}_n\\bigl(\n",
    "        x M + \\beta\\,,\\, \\mathrm{diag}(\\nu^2)\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "with $\\nu^2_j = \\sum_{i=1}^m \\sigma^2_{ij} x_i^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_linear_lrp(layer, input):\n",
    "    \"\"\"Forward pass for the linear layer with the local reparemetrization trick.\"\"\"\n",
    "\n",
    "    ## Exercise\n",
    "    # Get the mean\n",
    "    mu = F.linear(input, layer.weight, layer.bias)\n",
    "#     if not layer.training:\n",
    "#         # not deterministic pass even on `eval`\n",
    "#         return mu\n",
    "\n",
    "    # Add the resulting effect of weight randomness\n",
    "    s2 = F.linear(input * input, torch.exp(layer.log_sigma2), None)\n",
    "    return mu + torch.randn_like(s2) * torch.sqrt(s2 + 1e-20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer with Gaussian dropout and the trick "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the trick for the Gaussian approximation $q_\\theta(\\omega)$\n",
    "and one of the divergences, given above, into a stochastic linear layer:\n",
    "\n",
    "* we need write the forward pass and a parameter sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGaussian(torch.nn.Linear, VariationalModule):\n",
    "    \"\"\"Linear layer with Gaussian Mean Field weight distribution.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.log_sigma2 = torch.nn.Parameter(\n",
    "            torch.Tensor(*self.weight.shape))\n",
    "\n",
    "        self.reset_variational_parameters()\n",
    "\n",
    "    def reset_variational_parameters(self):\n",
    "        \"\"\"Initialize the log-variance.\"\"\"\n",
    "        self.log_sigma2.data.normal_(-8, 0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass with the local reparemetrization trick.\"\"\"\n",
    "\n",
    "        return stochastic_linear_lrp(self, input)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Return a sample from $q_{\\theta_m}(\\omega_m)$.\"\"\"\n",
    "        \n",
    "        ## Exercise\n",
    "        stdev = torch.exp(0.5 * self.log_sigma2)\n",
    "        weight = torch.normal(self.weight, std=stdev)\n",
    "\n",
    "        return {\"weight\": weight, \"bias\": self.bias}\n",
    "\n",
    "    @property\n",
    "    def penalty(self):\n",
    "        \"\"\"KL divergence between $q_{\\theta_m}(\\omega_m)$ an a prior on $\\omega_m$.\"\"\"\n",
    "\n",
    "        ## Exercise\n",
    "        # return kl_div_diffuse(self.log_sigma2, self.weight)\n",
    "        # return kl_div_scale_free(self.log_sigma2, self.weight)\n",
    "        # return kl_div_proper(self.log_sigma2, self.weight, nu=1e0)\n",
    "\n",
    "        return kl_div_1811_00596(self.log_sigma2, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer with Bernoulli Dropout and Gaussian approximation\n",
    "\n",
    "We can also fuse the classical Bernoulli dropout, [Hinton et al. 2012](https://arxiv.org/abs/1207.0580)\n",
    "with Gaussian variational approximation [Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of the fused model is simply a composition of dropout\n",
    "on inputs and the local reparameterization trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_bernoulli_lrp(layer, input, p=0.2):\n",
    "    \"\"\"Forward pass with Bernoulli dropout and the local reparemetrization trick.\"\"\"\n",
    "    input = F.dropout(input, p, layer.training)\n",
    "\n",
    "    return linear_with_lrp(layer, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for the Kullback-Leibler divergence we need to identify how\n",
    "$W$ are effectively distributed. Indeed, the variational approximation\n",
    "$q_{\\theta}(W)$ is essentially a spike-and-slab mixture: each row $W_i$\n",
    "of $W\\in \\mathbb{R}^{m \\times n}$ is either $\\mathbf{0}$ with probability\n",
    "$p$ or a Gaussian vector in $\\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "    W_i \\sim\n",
    "\\begin{cases}\n",
    "    \\mathbf{0}\n",
    "        & \\text{ w. prob } p \\\\\n",
    "    M_i\n",
    "        & \\text{ otherwise } \\\\\n",
    "\\end{cases}\n",
    "    \\,, \\text{indep.}\n",
    "    \\,, i=1, \\ldots, m\n",
    "    \\,, M_{ij} \\sim \\mathcal{N}_n\\bigl(\n",
    "        M_{ij} \\big\\vert \\mu_i, \\mathop{diag} \\sigma^2_i\n",
    "    \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under some assumptions and benign relaxations\n",
    "[Gal, Y. 2016 (eq. (6.3) p.109, Prop. 4 p.149)](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)\n",
    "the divergence can approximated by\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_i KL\\bigl( q_\\theta(W_i) \\big\\| p(W_i) \\bigr)\n",
    "        \\approx \\mathop{const}\n",
    "        % + \\frac{mn} 2 p \\{\\tau^{-1} + \\log \\tau\\}\n",
    "        % + m (p \\log p + (1-p) \\log (1-p))\n",
    "        + \\frac{1-p}2 \\sum_{ij}\n",
    "            \\sigma^2_{ij} + \\mu_{ij}^2 - \\log \\sigma^2_{ij}\n",
    "    \\,. $$\n",
    "\n",
    "<!--\n",
    "Proposition 4 p.149 is about the approximation of the divergence of\n",
    "a mixture from the standard Gaussian:\n",
    "\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\sum_k \\pi_k \\mathcal{N}_n(\\mu_k, \\Sigma_k)\n",
    "        \\big\\| \\mathcal{N}_n(0, I_n)\n",
    "    \\bigr)\n",
    "        \\approx \\mathop{const}\n",
    "        + \\frac12 \\sum_k \\pi_k \\{\n",
    "            \\mathop{tr}\\Sigma_k\n",
    "            + \\mu_k^\\top \\mu_k\n",
    "            - \\log\\det\\Sigma_k\n",
    "        \\}\n",
    "        - \\mathbb{H}(\\pi)\n",
    "    \\,. $$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to implement this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFusedGaussianBernoulli(LinearGaussian):\n",
    "    \"\"\"Linear layer with spike-slab (Gaussian) weight distribution.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.dropout(input, self.p, self.training)\n",
    "\n",
    "        return super().forward(input)\n",
    "\n",
    "    def sample(self):\n",
    "        p = torch.full_like(self.weight[:1], 1 - self.p)\n",
    "        mask = torch.bernoulli(p) / p\n",
    "\n",
    "        stdev = torch.exp(0.5 * self.log_sigma2)\n",
    "        weight = torch.normal(self.weight, std=stdev)\n",
    "        return {\n",
    "            \"weight\": weight * mask,\n",
    "            \"bias\": self.bias\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def penalty(self):\n",
    "        \"\"\"Approximate KL divergence.\"\"\"\n",
    "        return (1 - self.p) * kl_div_proper(self.log_sigma2, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/c10d24959b0ad615a21e671b180a1b2466d77a2b/keras/initializers.py#L341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a matrix $W \\in\\mathbb{R}^{\n",
    "[\\mathrm{out}]\n",
    "\\times [\\mathrm{in}]\n",
    "}$. Out and in are known as `fan-in` and `fan-out` respectively.\n",
    "\n",
    "Kaiming He:\n",
    "* (normal) $w_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\n",
    "    \\sigma = \\tfrac{\\sqrt2}{\\sqrt{\n",
    "        [\\mathrm{in}] (1+a^2)\n",
    "    }}\n",
    "$ ($a$ -- negative slope of the ReLU)\n",
    "* (uniform) $w_{ij} \\sim \\mathcal{U}[-\\sigma, +\\sigma]$ with $\n",
    "\\sigma = \\tfrac{\\sqrt6}{\\sqrt{\n",
    "        [\\mathrm{in}] (1+a^2)\n",
    "}}$\n",
    "\n",
    "Glorot Xavier:\n",
    "* (uniform) $w_{ij} \\sim \\mathcal{U}[-b, +b]$ with $\n",
    "b = \\mathrm{gain} \\cdot \\sqrt{\\tfrac6{\n",
    "        [\\mathrm{out}] + [\\mathrm{in}]\n",
    "    }}\n",
    "$\n",
    "* (normal) $w_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\n",
    "\\sigma = \\mathrm{gain} \\cdot \\sqrt{\\tfrac2{\n",
    "        [\\mathrm{out}] + [\\mathrm{in}]\n",
    "    }}\n",
    "$\n",
    "\n",
    "\n",
    "Based on this piece on [weight initialization](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.nn import init\n",
    "\n",
    "from torch.nn import Linear\n",
    "\n",
    "# class Linear(torch.nn.Linear):\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         init.xavier_uniform_(self.weight)\n",
    "#         if self.bias is not None:\n",
    "#             init.zeros_(self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    k: v for k, v in zip(\n",
    "        ['body.0.weight', 'body.0.bias',\n",
    "         'body.2.weight', 'body.2.bias',\n",
    "         'body.4.weight', 'body.4.bias'],\n",
    "        model.state_dict().values()\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(*train)\n",
    "\n",
    "feed = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "y_pred = torch.stack([\n",
    "    apply(model, test_feed) for _ in range(501)\n",
    "], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_std = y_pred.mean(dim=0), y_pred.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = canvas1d()\n",
    "\n",
    "ax.plot(X_test, y_test, lw=2, color=\"k\", alpha=0.25, label=\"test\")\n",
    "ax.plot(X_test, y_pred.numpy(), c=\"C0\", lw=2, label=\"predict\")\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"k\", s=20, label=\"train\")\n",
    "\n",
    "# confidence bands\n",
    "for m in [0.25, 0.50, 0.75, 1.00]:\n",
    "    y_hi = (y_pred + 2 * y_std * m).numpy()\n",
    "    y_lo = (y_pred - 2 * y_std * m).numpy()\n",
    "    plt.fill_between(X_test[:, 0], y_lo[:, 0], y_hi[:, 0],\n",
    "                     color=\"C3\", alpha=0.08)\n",
    "\n",
    "plt.legend(ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bands(X, mean, std, ax=None, bands=(0.50, 1.00, 1.50, 2.00), **kwargs):\n",
    "    ax = plt.gca() if ax is None else ax\n",
    "\n",
    "    for band in sorted(bands):\n",
    "        y_hi = (mean + std * band).numpy()\n",
    "        y_lo = (mean - std * band).numpy()\n",
    "        ax.fill_between(X[:, 0], y_lo[:, 0], y_hi[:, 0], **kwargs)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = canvas1d()\n",
    "\n",
    "ax.plot(X_test, y_test, lw=2, color=\"k\", alpha=0.25, label=\"test\")\n",
    "\n",
    "ax.plot(X_test, mean_sf.numpy(), c=\"C0\", lw=2, label=\"mean-sf\")\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"k\", s=20, label=\"train\")\n",
    "\n",
    "draw_bands(X_test, mean_sf, std_sf, ax=ax, color=\"C3\", alpha=0.08, zorder=-5)\n",
    "\n",
    "plt.legend(ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key issue with point-estimates is that values at each $x$ jointly do not\n",
    "coorespond to a function in the parameteric family modelled by the network:\n",
    "there may be no $\\omega \\in \\mathop{supp}q_\\theta(\\omega)$ such that\n",
    "$f_\\omega(x_i) = y_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
