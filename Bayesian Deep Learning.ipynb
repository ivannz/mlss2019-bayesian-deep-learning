{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLSS2019: Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn what basic building blocks are needed\n",
    "to endow (deep) neural networks with uncertainty estimates, and how\n",
    "this can be used in active learning or expert-in-the-loop pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan of the tutorial\n",
    "1. [Setup and imports](#Setup-and-imports)\n",
    "2. [Easy uncertainty in networks](#Easy-uncertainty-in-networks)\n",
    "   1. [Adding stochasticity](#Adding-stochasticity)\n",
    "   2. [Implementing function sampling with the DropoutLinear Layer](#Implementing-function-sampling-with-the-DropoutLinear-Layer)\n",
    "   3. [Implementing-DropoutLinear](#Implementing-DropoutLinear)\n",
    "   3. [Comparing sample functions to point-estimates](#Comparing-sample-functions-to-point-estimates)\n",
    "3. [A brief reminder on Bayesian and Variational Inference](#A-brief-reminder-on-Bayesian-and-Variational-Inference)\n",
    "4. [Bayesian Active Learning with images](#Bayesian-Active-Learning-with-images)\n",
    "   1. [the Acquisition Function](#the-Acquisition-Function)\n",
    "   2. [Dropout $2$-d Convolutional layer and the model](#Dropout-$2$-d-Convolutional-layer-and-the-model)\n",
    "   3. [Actively learning MNIST](#Actively-Learning-MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we import necessary modules and functions and\n",
    "define the computational device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install some boilerplate service code for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/ivannz/mlss2019-bayesian-deep-learning.git@developer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, numpy for computing, matplotlib for plotting and tqdm for progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning stuff will be using [pytorch](https://pytorch.org/).\n",
    "\n",
    "If you are unfamiliar with it, it is basically like `numpy` with autograd,\n",
    "native GPU support, and tools for building training and serializing models.\n",
    "<!-- (and with `axis` argument replaced with `dim` :) -->\n",
    "\n",
    "There are good introductory tutorials on `pytorch`, like this\n",
    "[one](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear, Conv2d\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need some functionality from scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the boilerplate code.\n",
    "\n",
    "* a procedure that implements a minibatch SGD **fit** loop\n",
    "* a function, that **evaluates** the model on the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# pseudocode\n",
    "def fit(model, dataset, criterion, ...):\n",
    "    for epoch in epochs:\n",
    "        for batch in dataset:\n",
    "            loss = criterion(model, batch)  # forward pass\n",
    "\n",
    "            grad = loss.backward()          # gradient via back propagation\n",
    "\n",
    "            adam_step(grad)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# pseudocode\n",
    "def predict(model, dataset, ...):\n",
    "    for input_batch in dataset:\n",
    "        output.append(model(input_batch))  # forward pass\n",
    "    \n",
    "    return concatenate(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy uncertainty in networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following model: a 3-layer fully connected\n",
    "network with LeakyReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    Linear(1, 512, bias=True),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    Linear(512, 512, bias=True),\n",
    "    torch.nn.LeakyReLU(),\n",
    "\n",
    "    Linear(512, 1, bias=True),\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the initial small dataset $S_0 = (x_i, y_i)_{i=1}^{m_0}$\n",
    "with $y_i = g(x_i)$, $x_i$ on a regular-spaced grid, and $\n",
    "g\n",
    "    \\colon \\mathbb{R} \\to \\mathbb{R}\n",
    "    \\colon x \\mapsto \\tfrac{x^2}4 + \\sin \\frac\\pi2 x\n",
    "$.\n",
    "<!--\n",
    "`dataset_from_numpy` **converts** numpy arrays into torch tensors,\n",
    "**places** them on the specified compute device, **and packages**\n",
    "into a dataset\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl import dataset_from_numpy\n",
    "\n",
    "X_train = np.linspace(-6.0, +6.0, num=20)[:, np.newaxis]\n",
    "y_train = np.sin(X_train * np.pi / 2) + 0.25 * X_train**2\n",
    "\n",
    "train = dataset_from_numpy(X_train, y_train, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_domain = np.linspace(-10., +10., num=251)[:, np.newaxis]\n",
    "\n",
    "domain = dataset_from_numpy(X_domain, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model on `train` using MSE loss and $\\ell_2$ penalty\n",
    "on weights (`weight_decay`):\n",
    "$$\n",
    "    \\tfrac1{2 m} \\|f_\\omega(x) - y\\|_2^2 + \\lambda \\|\\omega\\|_2^2\n",
    "    \\,, $$\n",
    "where $\\omega$ are all the learnable parameters of the model $f_\\omega(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(model, train, criterion=\"mse\", n_epochs=2000, verbose=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..., compute the predictions, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(model, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..., and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"black\", s=40, label=\"train\")\n",
    "\n",
    "ax.plot(X_domain, y_pred.numpy(), c=\"C0\", lw=2, label=\"prediction\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems to fit the train set adequately well. However, there is no\n",
    "way to assess how confident this model is with respect to its predictions.\n",
    "Indeed, the prediction $\\hat{y}_x = f_\\omega(x)$ is is a deterministic function\n",
    "of the input $x$ and the learnt parameters $\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding stochasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One inexpensive way to make any network into a stochastic function of its\n",
    "input is to add dropout before any parameterized layer like `linear`\n",
    "or `convolutional`, [Hinton et al. 2012](https://arxiv.org/abs/1207.0580).\n",
    "Essentially, dropout applies a Bernoulli mask to the features of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Gal, Y. (2016)](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)\n",
    "it has been shown that a simple, somewhat ad-hoc approach, of\n",
    "adding uncertainty quantification to networks through dropout,\n",
    "[Hinton et al. 2012](https://arxiv.org/abs/1207.0580),\n",
    "is a special case of Variational Inference.\n",
    "\n",
    "```\n",
    "A simple stochastic regularization method allows uncertainty estimation essentially for free!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Always Active Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For input\n",
    "$\n",
    "    x\\in \\mathbb{R}^{[\\mathrm{in}]}\n",
    "$ the dropout layer acts like this:\n",
    "\n",
    "$$\n",
    "    y_j = x_j \\, m_j\n",
    "    \\,, $$\n",
    "\n",
    "where $m\\in \\mathbb{R}^{[\\mathrm{in}]}$ with $\n",
    "m_j \\sim \\pi_p(m_j)\n",
    "    = \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)\n",
    "$,\n",
    "i.e. equals $\\tfrac1{1-p}$ with probability $1-p$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch` has a function for this `F.dropout(input, p, training)`. It multiplies\n",
    "each element of the `input` tensor by an independent Bernoulli rv. The argument\n",
    "`p` has the same meaning as above. The boolean argument `training` toggles the\n",
    "effect: if `False` then the input is returned as-is, otherwise the mask is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveDropout(torch.nn.Dropout):\n",
    "    # There is no need to redefine __init__(...), since\n",
    "    #  we are directly inheriting from `Dropout`.\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"We need to permanently latch the `training` toggle\n",
    "        to `True` in order to enable stochastic forward pass in\n",
    "        evaluation mode (`model.eval()`).\n",
    "        \"\"\"\n",
    "\n",
    "        ## Exercise: self.p - contains the specified dropout rate\n",
    "\n",
    "        return F.dropout(input, p=self.p, training=True)\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Rebuilding the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recreate the model above with this freshly minted dropout layer.\n",
    "Then fit and plot it's prediction uncertainty due to forward pass stochasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(p=0.5):\n",
    "    \"\"\"Build a model with dropout layers' rate set to `p`.\"\"\"\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        ## Exercise: Use ActiveDropout before the linear layers of\n",
    "        #  our first network. Note that dropping out input features\n",
    "        #  is not a good idea!\n",
    "\n",
    "        Linear(1, 512, bias=True),\n",
    "        torch.nn.LeakyReLU(),\n",
    "\n",
    "        ActiveDropout(p),\n",
    "        Linear(512, 512, bias=True),\n",
    "        torch.nn.LeakyReLU(),\n",
    "\n",
    "        ActiveDropout(p),\n",
    "        Linear(512, 1, bias=True),\n",
    "\n",
    "        # pass\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(p=0.5)\n",
    "model.to(device)\n",
    "\n",
    "fit(model, train, criterion=\"mse\", n_epochs=2000, verbose=True, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Sampling the random output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the test sample $\\tilde{S} = (\\tilde{x}_i)_{i=1}^m \\in \\mathcal{X}$\n",
    "and repeat the stochastic forward pass $B$ times at each $x\\in \\tilde{S}$:\n",
    "\n",
    "* for $b = 1 .. B$ do:\n",
    "\n",
    "  1. draw $y_{bi} \\sim f_\\omega(\\tilde{x}_i)$ for $i = 1 .. m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_estimate(model, dataset, n_samples=1, verbose=False):\n",
    "    \"\"\"Draw pointwise samples with stochastic forward pass.\"\"\"\n",
    "\n",
    "    ## Exercise: collect the random predictions over the dataset\n",
    "    ##  in a list, and then `stack` them into a B x m x d tensor,\n",
    "    ##  where d is the dimension of the prediction output.\n",
    "\n",
    "    outputs = []\n",
    "    for sample in tqdm.tqdm(range(n_samples), disable=not verbose):\n",
    "\n",
    "        outputs.append(predict(model, dataset))\n",
    "\n",
    "    return torch.stack(outputs, dim=0)\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "samples = point_estimate(model, domain, n_samples=101, verbose=True)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "samples.shape  # should be 101 x 251 x 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximate $95\\%$ confidence band of predictions is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"black\", s=40, label=\"train\")\n",
    "\n",
    "mean, std = samples.mean(dim=0).numpy(), samples.std(dim=0).numpy()\n",
    "ax.plot(X_domain, mean + 1.96 * std, c=\"k\")\n",
    "ax.plot(X_domain, mean - 1.96 * std, c=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the draws $y_{bi}$ as $B$ functional samples:\n",
    "$(x_i, y_{bi})_{i=1}^m$ - the $b$-th sample path.\n",
    "\n",
    "Below we plot $5$ random paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"black\", s=40, label=\"train\")\n",
    "ax.plot(X_domain[:, 0], samples[:5, :, 0].numpy().T, c=\"C0\", lw=1, alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that they are very erratic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing stochastic forward passes with a new mask each time is equivalent\n",
    "to drawing new **independent** prediction from for each point $x\\in \\tilde{S}$,\n",
    "without considering that, in fact, at adjacent points the predictions should\n",
    "be correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, is we were interested in uncertainty at some particular point,\n",
    "this would be okay: **fast and simple**. In contrast, if we were interested in\n",
    "the uncertainty of an integral **path-dependent** measure of the whole estimated\n",
    "function, or were doing **optimization** of the unknown true function taking\n",
    "estimation uncertainty into account, then this clearly erratic behaviour\n",
    "of paths is undesirable.\n",
    "Ex. see [blog: Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question(s) (to ponder in your spare time)\n",
    "\n",
    "* what will happen if you change the default dropout rate in `ActiveDropout` layer?\n",
    "  Try to rebuild the model with different $p \\in (0, 1)$ using `build_model(p)`,\n",
    "  and then plot the predictive bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing function sampling with the DropoutLinear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive implementation of `ActiveDropout` above defines the predictive\n",
    "distribution $y\\sim p(y \\mid x)$ as $y=f_\\omega(x; m)$ for $m \\sim \\pi_p(m)$,\n",
    "where $\\pi(m)$ denotes the distribution of Bernoulli dropout masks\n",
    "$\\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to implement some extra functionality on top of `pytorch`,\n",
    "in order to draw realizations from the induced distribution over\n",
    "functions, defined by a network, i.e. $\n",
    "\\bigl\\{\n",
    "    f_\\omega\\colon \\mathcal{X}\\to\\mathcal{Y}\n",
    "\\bigr\\}_{\\omega \\sim q(\\omega)}\n",
    "$\n",
    "where $q(\\omega)$ is a distribution over the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze/unfreeze interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a base **trait-class** `FreezableWeight` that adds\n",
    "interface for freezing and unfreezing layer's random **weight**\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreezableWeight(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.unfreeze()\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.register_buffer(\"frozen_weight\", None)\n",
    "\n",
    "    def is_frozen(self):\n",
    "        \"\"\"Check if a frozen weight is available.\"\"\"\n",
    "        return isinstance(self.frozen_weight, torch.Tensor)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"\"\"Sample from the distribution and freeze.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we declare a pair of functions:\n",
    "* `freeze()` instructs a compatible layer (module) to sample and freeze its randomness\n",
    "* `unfreeze()` requests the layer to undo this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze(module):\n",
    "    for mod in module.modules():\n",
    "        if isinstance(mod, FreezableWeight):\n",
    "            mod.unfreeze()\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    for mod in module.modules():\n",
    "        if isinstance(mod, FreezableWeight):\n",
    "            mod.freeze()\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling realizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm to sample a random function is:\n",
    "* for $b = 1... B$ do:\n",
    "\n",
    "  1. draw an independent realization $f_b\\colon \\mathcal{X} \\to \\mathcal{Y}$\n",
    "  with from the process $\\{f_\\omega\\}_{\\omega \\sim q(\\omega)}$\n",
    "  2. get $\\hat{y}_{bi} = f_b(\\tilde{x}_i)$ for $i=1 .. m$\n",
    "\n",
    "\n",
    "* compute mean and variance of $\\hat{y}_{bi}$ along $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_function(model, dataset, n_samples=1, verbose=False):\n",
    "    \"\"\"Draw a realization of a random function.\"\"\"\n",
    "\n",
    "    ## Exercise: collect the `frozen` predictions over the dataset\n",
    "    ##  in a list, and then `stack` them into a B x m x d tensor,\n",
    "    ##  where d is the dimension of the prediction output.\n",
    "\n",
    "    outputs = []\n",
    "    for _ in tqdm.tqdm(range(n_samples), disable=not verbose):\n",
    "\n",
    "        outputs.append(predict(freeze(model), dataset))\n",
    "\n",
    "    unfreeze(model)\n",
    "\n",
    "    return torch.stack(outputs, dim=0)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** although the internal loop in both functions looks\n",
    "similar they, conceptually the functions differ:\n",
    "<strong>\n",
    "```python\n",
    "def point_estimate(f, S):\n",
    "    for x in S:\n",
    "        for w in f.q:\n",
    "            yield f(x, w)\n",
    "\n",
    "\n",
    "def sample_function(f, S):\n",
    "    for w in f.q:  # thanks to freeze\n",
    "        for x in S:\n",
    "            yield f(x, w)\n",
    "```\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing `DropoutLinear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge `Dropout` and `Linear` layers into one, which\n",
    "\n",
    "1. (on forward pass) **drops out** the inputs, if necessary, and **applies** the linear (affine) transform\n",
    "2. (on freeze) **randomly zeros** columns in a copy of the the weight matrix $W$\n",
    "\n",
    "Preferably, we will try to preserve interface, so that the resulting\n",
    "object is backwards compatible with `Linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we would be able to draw realizations from the induced\n",
    "distribution over functions defined by the network $\n",
    "\\bigl\\{\n",
    "    f_\\omega\\colon \\mathcal{X}\\to\\mathcal{Y}\n",
    "\\bigr\\}_{\\omega \\sim q(\\omega)}\n",
    "$\n",
    "where $q(\\omega)$ a distribution over the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Fused dropout-linear operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the inputs into a linear layer dropout acts like this: for input\n",
    "$\n",
    "    x\\in \\mathbb{R}^{[\\mathrm{in}]}\n",
    "$ and layer weights $\n",
    "    W\\in \\mathbb{R}^{[\\mathrm{out}] \\times [\\mathrm{in}]}\n",
    "$\n",
    "and bias $\n",
    "    b\\in \\mathbb{R}^{[\\mathrm{out}]}\n",
    "$ the resulting effect is\n",
    "\n",
    "$$\n",
    "    \\tilde{x} = x \\odot m\n",
    "    \\,, \\\\\n",
    "    y = \\tilde{x} W^\\top + b\n",
    "%     = b + \\sum_i x_i m_i W_i\n",
    "    \\,, $$\n",
    "\n",
    "where $\\odot$ is the elementwise product and $m\\in \\mathbb{R}^{[\\mathrm{in}]}$\n",
    "with $m_j \\sim \\pi_p(m_j) = \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$,\n",
    "i.e. equals $\\tfrac1{1-p}$ with probability $1-p$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "$\n",
    "    x\\in \\mathbb{R}^{[\\mathrm{in}]}\n",
    "$, $\n",
    "    W\\in \\mathbb{R}^{[\\mathrm{out}] \\times [\\mathrm{in}]}\n",
    "$\n",
    "and $\n",
    "    b\\in \\mathbb{R}^{[\\mathrm{out}]}\n",
    "$\n",
    "* `F.dropout(x, p, on/off)` -- Bernoulli dropout $x\\mapsto x\\odot m$\n",
    "  for $m\\sim \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$\n",
    "* `F.linear(x, W, b)` -- affine transformation $x \\mapsto x W^\\top + b$\n",
    "\n",
    "**(NOTE)** the weight of a linear layer in `pytorch` is $\n",
    "{\n",
    "    [\\mathrm{out}]\n",
    "    \\times [\\mathrm{in}]\n",
    "}\n",
    "$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropoutLinear_forward(self, input):\n",
    "    ## Exercise: If not frozen, then apply always active dropout,\n",
    "    #  then linear transformation. If frozen, apply the transform\n",
    "    #  using the frozen weight\n",
    "\n",
    "    # linear with frozen weight\n",
    "    if self.is_frozen():\n",
    "        return F.linear(input, self.frozen_weight, self.bias)\n",
    "\n",
    "    # stochastic pass as in `ActiveDropout` + Linear\n",
    "    input = F.dropout(input, self.p, True)\n",
    "\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "    # return super().forward(F.dropout(input, self.p, True))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter freezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For input\n",
    "$\n",
    "    x\\in \\mathbb{R}^{[\\mathrm{in}]}\n",
    "$ and a layer parameters $\n",
    "    W\\in \\mathbb{R}^{[\\mathrm{out}] \\times [\\mathrm{in}]}\n",
    "$\n",
    "and $\n",
    "    b\\in \\mathbb{R}^{[\\mathrm{out}]}\n",
    "$ the effect in `DropoutLinear` is\n",
    "\n",
    "$$\n",
    "    y_j\n",
    "        = \\bigl[(x \\odot m) W^\\top + b\\bigr]_j\n",
    "        = b_j + \\sum_i x_i m_i W_{ji}\n",
    "        = b_j + \\sum_i x_i \\breve{W}_{ji}\n",
    "    \\,, $$\n",
    "\n",
    "where the each column of $\\breve{W}_i$ is, independently, either\n",
    "$\\mathbf{0} \\in \\mathbb{R}^{[\\mathrm{out}]}$ with probability $p$ or\n",
    "some (learnable) vector in $\\mathbb{R}^{[\\mathrm{out}]}$\n",
    "\n",
    "$$\n",
    "    \\breve{W}_i \\sim\n",
    "\\begin{cases}\n",
    "    \\mathbf{0}\n",
    "        & \\text{ w. prob } p \\,, \\\\\n",
    "    \\tfrac1{1-p} M_i\n",
    "        & \\text{ w. prob } 1-p \\,.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus the multiplicative effect of the random mask $m$ on $x$ can be\n",
    "equivalently seen as a random **on/off** switch effect on the\n",
    "**columns** of the matrix $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropoutLinear_freeze(self):\n",
    "    \"\"\"Apply dropout with rate `p` to columns of `weight` and freeze it.\"\"\"\n",
    "    # we leverage torch's broadcasting semantics and draw a one-row\n",
    "    #  mask binary mask, that we later multiply the weight by.\n",
    "\n",
    "    # let's draw the new weight\n",
    "    prob = torch.full_like(self.weight[:1, :], 1 - self.p)\n",
    "    feature_mask = torch.bernoulli(prob) / prob\n",
    "\n",
    "    frozen_weight = self.weight * feature_mask\n",
    "\n",
    "    # and store it\n",
    "    self.register_buffer(\"frozen_weight\", frozen_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)**\n",
    "The parameter distribution of the layer we're building is\n",
    "\n",
    "$$\n",
    "    q(W)\n",
    "        = \\prod_i q(W_i)\n",
    "        = \\prod_i \\bigl\\{\n",
    "            p \\delta_{\\mathbf{0}} (W_i)\n",
    "            + (1 - p) \\delta_{\\tfrac1{1-p} M_i}(W_i)\n",
    "        \\bigr\\}\n",
    "    \\,, $$\n",
    "\n",
    "where $W_i$ is the $i$-th column of $W$ and $\\delta_x$ is a\n",
    "**point-mass** distribution at $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutLinear(Linear, FreezableWeight):\n",
    "    \"\"\"Linear layer with dropout on inputs.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    forward = DropoutLinear_forward\n",
    "\n",
    "    freeze = DropoutLinear_freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing sample functions to point-estimates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite the model builder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(p=0.5):\n",
    "    \"\"\"Build a model with dropout layers' rate set to `p`.\"\"\"\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "        ## Exercise: Plug-in `DropoutLinear` layer into our second network.\n",
    "\n",
    "        Linear(1, 512, bias=True),\n",
    "        torch.nn.LeakyReLU(),\n",
    "\n",
    "        DropoutLinear(512, 512, bias=True , p=p),\n",
    "        torch.nn.LeakyReLU(),\n",
    "\n",
    "        DropoutLinear(512, 1, bias=True, p=p),\n",
    "\n",
    "        # pass\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new instance and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(p=0.5)\n",
    "model.to(device)\n",
    "\n",
    "fit(model, train, criterion=\"mse\", n_epochs=2000, verbose=True, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and obtain two estimates: pointwise and functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pe = point_estimate(model, domain, n_samples=51, verbose=True)\n",
    "samples_sf = sample_function(model, domain, n_samples=51, verbose=True)\n",
    "\n",
    "samples_pe.shape, samples_sf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare <span style=\"color:#1f77b4\">**point estimates**</span>\n",
    "with <span style=\"color:#ff7f0e\">**function sampling**</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax.plot(X_domain[:, 0], samples_pe[:15, :, 0].numpy().T,\n",
    "        c=\"C1\", lw=1, alpha=0.5)\n",
    "\n",
    "ax.plot(X_domain[:, 0], samples_sf[:15, :, 0].numpy().T,\n",
    "        c=\"C0\", lw=2, alpha=0.5)\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"black\", s=40,\n",
    "           label=\"train\", zorder=+10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax.scatter(X_train, y_train, c=\"black\", s=40, label=\"train\")\n",
    "\n",
    "mean, std = samples_sf.mean(dim=0).numpy(), samples_sf.std(dim=0).numpy()\n",
    "ax.plot(X_domain, mean + 1.96 * std, c=\"C0\")\n",
    "ax.plot(X_domain, mean - 1.96 * std, c=\"C0\");\n",
    "\n",
    "mean, std = samples_pe.mean(dim=0).numpy(), samples_pe.std(dim=0).numpy()\n",
    "ax.plot(X_domain, mean + 1.96 * std, c=\"C1\")\n",
    "ax.plot(X_domain, mean - 1.96 * std, c=\"C1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of `point-estimate`:\n",
    "* uses stochastic forward passes -- no need to for extra code and classes\n",
    "\n",
    "Cons of `point-estimate`:\n",
    "* predictive distributions at adjacent inputs are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question(s) (to ponder in your spare time)\n",
    "\n",
    "* what happens when you increase the number of samples path-wise and pointwise,\n",
    "  and inspect their statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief reminder on Bayesian and Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Inference is a principled framework of reasoning about uncertainty.\n",
    "\n",
    "In Bayesian Inference (**BI**) we *assume* that the observation\n",
    "data $D$ follows a *model* $m$ with data generating distribution\n",
    "$p(D\\mid m, \\omega)$ *governed by unknown parameters* $\\omega$.\n",
    "The goal of **BI** is to reason about the model and/or its parameters,\n",
    "and new data given the observed data $D$ and our assumptions, i.e\n",
    "to seek the **posterior** parameter and predictive distributions:\n",
    "\n",
    "$$\\begin{align}\n",
    "    p(d \\mid D, m)\n",
    "        % &= \\mathbb{E}_{\n",
    "        %     \\omega \\sim p(\\omega \\mid D, m)\n",
    "        % } p(d \\mid D, \\omega, m)\n",
    "        &= \\int p(d \\mid D, \\omega, m) p(\\omega \\mid D, m) d\\omega\n",
    "    \\,, \\\\\n",
    "    p(\\omega \\mid D, m)\n",
    "        &= \\frac{p(D\\mid \\omega, m) \\, \\pi(\\omega \\mid m)}{p(D\\mid m)}\n",
    "    \\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* the **prior** distribution $\\pi(\\omega \\mid m)$ reflects our belief\n",
    "  before having made the observations\n",
    "\n",
    "* the data distribution $p(D \\mid \\omega, m)$ reflects our assumptions\n",
    "  about the data generating process, and determines the parameter\n",
    "  **likelihood** (Gaussian, Categorical, Poisson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless the distributions and likelihoods are conjugate, posterior in\n",
    "Bayesian inference is typically intractable and it is common to resort\n",
    "to **Variational Inference** or **Monte Carlo** approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This key idea of this approach is to seek an approximation $q(\\omega)$\n",
    "to the intractable posterior $p(\\omega \\mid D, m)$, via a variational\n",
    "optimization problem over some tractable family of distributions $\\mathcal{Q}$:\n",
    "\n",
    "$$\n",
    "    q^*(\\omega)\n",
    "        \\in \\arg \\min_{q\\in \\mathcal{Q}} \\mathrm{KL}(q(\\omega) \\| p(\\omega \\mid D, m))\n",
    "    \\,, $$\n",
    "\n",
    "where the Kullback-Leibler divergence between $P$ and $Q$ ($P\\ll Q$)\n",
    "with densities $p$ and $q$, respectively, is given by\n",
    "\n",
    "$$\n",
    "    \\mathrm{KL}(q(\\omega) \\| p(\\omega))\n",
    "%         = \\mathbb{E}_{\\omega \\sim Q} \\log \\tfrac{dQ}{dP}(\\omega)\n",
    "        = \\mathbb{E}_{\\omega \\sim q(\\omega)}\n",
    "            \\log \\tfrac{q(\\omega)}{p(\\omega)}\n",
    "    \\,. \\tag{kl-div} $$\n",
    "\n",
    "\n",
    "Note that the family of variational approximations $\\mathcal{Q}$ can be\n",
    "structured **arbitrarily**: point-mass, products, mixture, dependent on\n",
    "input, having mixed hierarchical structure, -- any valid distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although computing the divergence w.r.t. the unknown posterior\n",
    "is still hard and intractable, it is possible to do away with it\n",
    "through the following identity, which is based on the Bayes rule.\n",
    "\n",
    "For **any** $q(\\omega) \\ll p(\\omega \\mid D; \\phi)$ and any model $m$\n",
    "\n",
    "$$\n",
    "    \\overbrace{\n",
    "        \\log p(D \\mid m)\n",
    "    }^{\\text{evidence}}\n",
    "        = \\underbrace{\n",
    "            \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega, m)\n",
    "        }_{\\text{expected conditional likelihood}}\n",
    "        - \\overbrace{\n",
    "            \\mathrm{KL}(q(\\omega)\\| \\pi(\\omega \\mid m))\n",
    "        }^{\\text{proximity to prior belief}}\n",
    "        + \\underbrace{\n",
    "            \\mathrm{KL}(q(\\omega)\\| p(\\omega \\mid D, m))\n",
    "        }_{\\text{posterior approximation}}\n",
    "    \\,. \\tag{master-identity} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can solve an equivalent maximization problem with respect to $q(\\omega)$:\n",
    "\n",
    "$$\n",
    "    q^* \\in\n",
    "    \\arg\\max_{q\\in Q}\n",
    "        \\mathrm{ELBO}(q) = \n",
    "            \\mathbb{E}_{\\omega \\sim q} \\log p(D\\mid \\omega, m)\n",
    "            - \\mathrm{KL}(q(\\omega)\\| \\pi(\\omega \\mid m))\n",
    "    \\,. $$\n",
    "\n",
    "* the expected likelihood -- favours $q$ that place their mass on\n",
    "parameters $\\omega$ that explain the observed data under the specified\n",
    "model $m$.\n",
    "\n",
    "* the negative KL-divergence -- encourages variational densities\n",
    "not to stray away too far from to the prior belief under the model $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the assumed likelihood $p(D \\mid \\omega, m)$ and the prior $\\pi(\\omega\\mid m)$\n",
    "have their own parameters $\\phi$, then the lower bound\n",
    "\n",
    "$$\n",
    "    \\log p_\\phi(D \\mid m)\n",
    "        \\geq \\mathrm{ELBO}(q, \\phi)\n",
    "            = \\mathbb{E}_{\\omega \\sim q(\\omega)} \\log p_\\phi(D \\mid \\omega, m)\n",
    "            - \\mathbb{E}_{\\omega \\sim q(\\omega)} \\log \\frac{q(\\omega)}{\\pi_\\phi(\\omega)}\n",
    "    \\,, $$\n",
    "\n",
    "naturaly yields a coordinate-wise ascent algorithm:\n",
    "* **(E)** step wrt $q$, fixed $\\phi$\n",
    "* **(M)** step wrt $\\phi$, fixed $q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the variational approximation yields high dimensional\n",
    "integrals, that are computationally heavy. To make the computations\n",
    "faster without foregoing much of precision, we may use sampling,\n",
    "or Monte Carlo methods. For the predictive distribution, for example,\n",
    "we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{y\\sim p(y\\mid x, D, m)} \\, g(y)\n",
    "        &\\overset{\\text{BI}}{=}\n",
    "            \\mathbb{E}_{\\omega\\sim p(\\omega \\mid D, m)}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega, D, m)} \\, g(y) \n",
    "        \\\\\n",
    "        &\\overset{\\text{VI}}{\\approx}\n",
    "            \\mathbb{E}_{\\omega\\sim q(\\omega)}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega, D, m)} \\, g(y)\n",
    "        \\\\\n",
    "        &\\overset{\\text{MC}}{\\approx}\n",
    "%             \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "%                 \\mathbb{E}_{y\\sim p(y\\mid x, \\omega, D, m)} \\, g(y)\n",
    "            \\frac1{\\lvert \\mathcal{W}\\rvert} \\sum_{\\omega \\in \\mathcal{W}}\n",
    "                \\mathbb{E}_{y\\sim p(y\\mid x, \\omega, D, m)} \\, g(y)\n",
    "    \\,,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W} = (\\omega_b)_{b=1}^B \\sim q(\\omega)$\n",
    "-- iid samples from the variational approximation.\n",
    "\n",
    "**(note)** If $p(y \\mid x, \\omega, D, m)$ yield \"heavy\" integrals then\n",
    "we apply Monte Carlo to it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good summary of Bayesian Inference can be found in [this lecture](http://mlg.eng.cam.ac.uk/zoubin/talks/lect1bayes.pdf), [this paper](https://arxiv.org/abs/1206.7051.pdf), [this review](https://arxiv.org/abs/1601.00670.pdf).\n",
    "It is also possible to consult [wiki](https://en.wikipedia.org/wiki/Bayesian_inference) and references therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Active Learning with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data labelling is costly and time consuming\n",
    "* unlabeled instances are essentially free\n",
    "\n",
    "**Goal** Achieve high performance with fewer labels by\n",
    "identifying the best instances to learn from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential blocks of active learning:\n",
    "\n",
    "* a **model** $m$ capable of quantifying uncertainty (preferably a Bayesian model)\n",
    "* an **acquisition function** $a\\colon \\mathcal{M} \\times \\mathcal{X}^* \\to \\mathbb{R}$\n",
    "  that for any finite set of inputs $S\\subset \\mathcal{X}$ quantifies their usefulness\n",
    "  to the model $m\\in \\mathcal{M}$\n",
    "* a labelling **oracle**, e.g. a human expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop of active learning:\n",
    "\n",
    "1. fit $m$ on $\\mathcal{S}_{\\mathrm{labelled}}$\n",
    "\n",
    "2. get exact (or approximate) $$\n",
    "    \\mathcal{S}^* \\in \\arg \\max\\limits_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "        a(m, S)\n",
    "$$ satisfying **budget constraints** and **without** access to targets\n",
    "(constraints, like $\\lvert S \\rvert \\leq \\ell$ or other economically motivated ones).\n",
    "\n",
    "3. request the **oracle** to provide labels for each $x\\in \\mathcal{S}^*$\n",
    "\n",
    "4. update $\n",
    "\\mathcal{S}_{\\mathrm{labelled}}\n",
    "    \\leftarrow \\mathcal{S}^*\n",
    "        \\cup \\mathcal{S}_{\\mathrm{labelled}}\n",
    "$ and goto 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a Bayesian model that can be used to reason\n",
    "about uncertainty, so let's focus on the acquisition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Acquisition Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many acquisition criteria (borrowed from [Gal17a](http://proceedings.mlr.press/v70/gal17a.html)):\n",
    "* Classification\n",
    "  * Max entropy (plain uncertainty)\n",
    "  * Maximal information about parameters and predictions (mutual information)\n",
    "  * Variance ratios\n",
    "  * Mean standard deviation\n",
    "  * **BALD**\n",
    "* Regression\n",
    "  * predictive variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BALD** (Bayesian Active Learning by Disagreement) acquisition\n",
    "criterion is based on the posterior mutual information between model's\n",
    "predictions $y_x$ at some point $x$ and model's parameters $\\omega$:\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\sum_{x\\in S} a(m, \\{x\\})\n",
    "        \\\\\n",
    "    a(m, \\{x\\})\n",
    "        &= \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "\\end{align}\n",
    "    \\,, \\tag{bald} $$\n",
    "\n",
    "with the [**Mutual Information**](https://en.wikipedia.org/wiki/Mutual_information#Relation_to_Kullback%E2%80%93Leibler_divergence)\n",
    "(**MI**)\n",
    "$$\n",
    "    \\mathbb{I}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m, D)}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi} $$\n",
    "\n",
    "and the [(differential) **entropy**](https://en.wikipedia.org/wiki/Differential_entropy#Differential_entropies_for_various_distributions)\n",
    "(all densities and/or probability mass functions can be conditional):\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p} \\log p(y)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the exact formula for **MI** we shall use its **Monte Carlo** (**MC**)\n",
    "approximation, since the expectations are analytically or numerically\n",
    "tractable only in simple low dimensional cases.\n",
    "<!-- probability integrals are still integrals -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an iid sample $\\mathcal{W} = (\\omega_b)_{b=1}^B \\sim q(\\omega \\mid m, D)$\n",
    "of size $B$. The **MC** approximation of the mutual information is\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{MC}(y_x; \\omega \\mid x, m, D)\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "        \\bigr)\n",
    "        - \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "            \\mathbb{H}\\bigl(\n",
    "                p(y_x \\,\\mid\\, x, \\omega, m, D)\n",
    "            \\bigr)\n",
    "    \\,, \\tag{mi-mc} $$\n",
    "\n",
    "where $\\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}} h(\\omega) = \\tfrac1B \\sum_j h(\\omega_j)$\n",
    "denotes the expectation with respect to the empirical probability measure induced\n",
    "by the sample $\\mathcal{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical (discrete) random variables $y \\sim \\mathcal{Cat}(\\mathbf{p})$,\n",
    "$\\mathbf{p} \\in \\{ \\mu \\in [0, 1]^d \\colon \\sum_k \\mu_k = 1\\}$, the entropy is\n",
    "\n",
    "$$\n",
    "    \\mathbb{H}(p(y))\n",
    "        = - \\mathbb{E}_{y\\sim p(y)} \\log p(y)\n",
    "        = - \\sum_k p_k \\log p_k\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** although in calculus $0 \\cdot \\log 0 = 0$ (because\n",
    "$\\lim_{p\\downarrow 0} p \\cdot \\log p = 0$), in floating point\n",
    "arithmetic $0 \\cdot \\log 0 = \\mathrm{NaN}$. So you need to add\n",
    "some **really tiny float number** to the argument of $\\log$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(proba):\n",
    "    \"\"\"Compute the entropy along the last dimension.\"\"\"\n",
    "\n",
    "    ## Exercise: get the entropy of a tensor with distributions\n",
    "    #  along the last axis.\n",
    "\n",
    "    return - torch.kl_div(torch.tensor(0.).to(proba), proba).sum(dim=-1)\n",
    "    return - torch.sum(proba * torch.log(proba + 1e-20), dim=-1)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a tensor $p_{bik}$ of probabilities $p(y_{x_i}=k \\mid x_i, \\omega_b, m, D)$\n",
    "with $\\omega_b \\sim q(\\omega \\mid m, D)$.\n",
    "\n",
    "Let's implement a procedure that computes the **MC** estimate \n",
    "of the posterior predictive distribution\n",
    "\n",
    "$$\n",
    "\\hat{p}(y_x\\mid x, m, D)\n",
    "    = \\hat{\\mathbb{E}}_{\\omega \\sim \\mathcal{W}}\n",
    "        \\,p(y_x \\mid x, \\omega, m, D)\n",
    "    \\,, $$\n",
    "\n",
    "its **entropy** $\n",
    "    \\mathbb{H}\\bigl(\\hat{p}(y\\mid x, m, D)\\bigr)\n",
    "$ and **mutual information** $\n",
    "    \\mathbb{I}_\\mathrm{MC}(y_x ; \\omega\\mid x, m, D)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(proba):\n",
    "    ## Exercise: compute a Monte Carlo estimator of the predictive\n",
    "    ##   distribution, its entropy and MI `H E_w p(., w) - E_w H p(., w)`\n",
    "\n",
    "    proba_avg = proba.mean(dim=0)\n",
    "\n",
    "    entropy_expected = entropy(proba_avg)\n",
    "    expected_entropy = entropy(proba).mean(dim=0)\n",
    "\n",
    "    mut_info = entropy_expected - expected_entropy\n",
    "\n",
    "    pass\n",
    "\n",
    "    return proba_avg, entropy_expected, mut_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) implementing BALD acqustion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acquisition function that we will implement takes in the\n",
    "sample mutual information and returns the indices of selected\n",
    "points.\n",
    "\n",
    "\n",
    "\n",
    "Note that $a(m, S)$ is additively separable, i.e. equals $\\sum_{x\\in S} a(m, \\{x\\})$.\n",
    "This implies that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\max_{S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}} a(m, S)\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "            \\sum_{x\\in F \\cup \\{x\\}} a(m, \\{x\\})\n",
    "        \\\\\n",
    "        &= \\max_{z \\in \\mathcal{S}_\\mathrm{unlabelled}}\n",
    "            a(m, \\{z\\})\n",
    "            + \\max_{F \\in \\mathcal{S}_\\mathrm{unlabelled} \\setminus \\{z\\}}\n",
    "                \\sum_{x\\in F} a(m, \\{x\\})\n",
    "\\end{align}\n",
    "    \\,. $$\n",
    "\n",
    "Therefore selecting the $\\ell$ `most interesting` points from $\\mathcal{S}_\\mathrm{unlabelled}$\n",
    "is trivial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acq_bald(mutual_info, n_points=10):\n",
    "    ## Exercise: implement the acquisition\n",
    "\n",
    "    indices = mutual_info.argsort()\n",
    "\n",
    "    return indices[-n_points:]\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** A drawback of the `pointwise` top-$\\ell$ procedure above is\n",
    "that, although it acquires individually informative instances, altogether\n",
    "they might end up **being** `jointly poorly informative`. This can be\n",
    "corrected if we would seek the highest mutual information among finite\n",
    "sets $S \\subseteq \\mathcal{S}_\\mathrm{unlabelled}$ of size $\\ell$.\n",
    "Such acquisition function is called **batch-BALD**\n",
    "([Kirsch et al.; 2019](https://arxiv.org/abs/1906.08158.pdf)):\n",
    "\n",
    "$$\\begin{align}\n",
    "    a(m, S)\n",
    "        &= \\mathbb{I}\\bigl((y_x)_{x\\in S}; \\omega \\mid S, m \\bigr)\n",
    "        = \\mathbb{H} \\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q(\\omega\\mid m)} H\\bigl(\n",
    "            p\\bigl((y_x)_{x\\in S}\\mid S, \\omega, m \\bigr)\n",
    "        \\bigr)\n",
    "\\end{align}\n",
    "    \\,. \\tag{batch-bald} $$\n",
    "\n",
    "This criterion requires exponentially large number of computations and\n",
    "memory, however there are working solutions like random sampling of subsets\n",
    "$\\mathcal{S}$ of size $\\ell$ from $\\mathcal{S}_\\mathrm{unlabelled}$ or\n",
    "greedy maximization of this *submodular* criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kirsch, A., van Amersfoort, J., & Gal, Y. (2019). BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. arXiv preprint [arXiv:1906.08158](https://arxiv.org/abs/1906.08158.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task*) Unbiased estimator of entropy and mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the **MC** estimate of the mutual information is the\n",
    "so-called **plug-in** estimator of the entropy:\n",
    "\n",
    "$$\n",
    "    \\hat{H}\n",
    "        = \\mathbb{H}(\\hat{p}) = - \\sum_k \\hat{p}_k \\log \\hat{p}_k\n",
    "    \\,, $$\n",
    "\n",
    "where $\\hat{p}_k = \\tfrac1B \\sum_b p_{bk}$ is the full sample estimator\n",
    "of the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that this plug-in estimate is biased\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html)\n",
    "and references therein, also this [notebook](https://colab.research.google.com/drive/1z9ZDNM6NFmuFnU28d8UO0Qymbd2LiNJW)). <!--($\\log$ + Jensen)-->\n",
    "In order to correct for small-sample bias we can use\n",
    "[jackknife resampling](https://en.wikipedia.org/wiki/Jackknife_resampling).\n",
    "It derives an estimate of the finite sample bias from the leave-one-out\n",
    "estimators of the entropy and is relatively computationally cheap\n",
    "(see [blog: Nowozin, 2015](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html),\n",
    "[Miller, R. G. (1974)](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/references/Miller74jackknife.pdf) and these [notes](http://people.bu.edu/aimcinto/jackknife.pdf)).\n",
    "\n",
    "The jackknife correction of a plug-in estimator $\\mathbb{H}(\\cdot)$\n",
    "is computed thus: given a sample $(p_b)_{b=1}^B$ with $p_b$ -- discrete distribution on $1..K$\n",
    "* for each $b=1.. B$\n",
    "  * get the leave-one-out estimator: $\\hat{p}_k^{-b} = \\tfrac1{B-1} \\sum_{j\\neq b} p_{jk}$\n",
    "  * compute the plug-in entropy estimator: $\\hat{H}_{-b} = \\mathbb{H}(\\hat{p}^{-b})$\n",
    "* then compute the bias-corrected entropy estimator $\n",
    "\\hat{H}_J\n",
    "    = \\hat{H} + (B - 1) \\bigl\\{\n",
    "        \\hat{H} - \\tfrac1B \\sum_b \\hat{H}^{-b}\n",
    "    \\bigr\\}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** when we knock the $i$-th data point out of the sample mean\n",
    "$\\mu = \\tfrac1n \\sum_i x_i$ and recompute the mean $\\mu_{-i}$ we get\n",
    "the following relation\n",
    "$$ \\mu_{-i}\n",
    "    = \\frac1{n-1} \\sum_{j\\neq i} x_j\n",
    "    = \\frac{n}{n-1} \\mu - \\tfrac1{n-1} x_i\n",
    "    = \\mu + \\frac{\\mu - x_i}{n-1}\n",
    "    \\,. $$\n",
    "This makes it possible to quickly compute leave-one-out estimators of\n",
    "discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    def mutual_information(proba):\n",
    "        ## Exercise: MC estimate of the predictive distribution, entropy and MI\n",
    "        ##  mutual information `H E_w p(., w) - E_w H p(., w)` with jackknife\n",
    "        ##  correction.\n",
    "\n",
    "        proba_avg = proba.mean(dim=0)\n",
    "\n",
    "        # plug-in estimate of entropy\n",
    "        entropy_expected = entropy(proba_avg)\n",
    "\n",
    "        # jackknife correction\n",
    "        proba_loo = proba_avg + (proba_avg - proba) / (len(proba) - 1)\n",
    "\n",
    "        expected_entropy_loo = entropy(proba_loo).mean(dim=0)\n",
    "        entropy_expected += (len(proba) - 1) * (entropy_expected - expected_entropy_loo)\n",
    "\n",
    "        # expected entropy is unbiased\n",
    "        expected_entropy = entropy(proba).mean(dim=0)\n",
    "\n",
    "        mut_info = entropy_expected - expected_entropy\n",
    "\n",
    "        pass\n",
    "\n",
    "        return proba_avg, entropy_expected, mut_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout $2$-d Convolutional layer and the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, in convolutional neural networks the dropout acts upon the feature\n",
    "(channel) information and not on the spatial dimensions. Thus entire channels\n",
    "are dropped out and for $\n",
    "    x \\in \\mathbb{R}^{\n",
    "        [\\mathrm{in}]\n",
    "        \\times h\n",
    "        \\times w}\n",
    "$ and $\n",
    "    y \\in \\mathbb{R}^{\n",
    "        [\\mathrm{out}]\n",
    "        \\times h'\n",
    "        \\times w'}\n",
    "$ the full effect of the `Dropout+Conv2d` layer is\n",
    "\n",
    "$$\n",
    "    y_{lij} = ((x \\odot m) \\ast W_l)_{ij} + b_l\n",
    "        = b_l + \\sum_k \\sum_{pq} x_{k i_p j_q} m_k W_{lkpq}\n",
    "    \\,, \\tag{conv-2d} $$\n",
    "    \n",
    "where i.i.d $m_k \\sim \\mathcal{Ber}\\bigl(\\bigl\\{0, \\tfrac1{1-p}\\bigr\\}, 1-p\\bigr)$,\n",
    "and indices $i_p$ and $j_q$ represent the spatial location in $x$ that correspond\n",
    "to the $p$ and $q$ elements in the kernel $\n",
    "    W\\in \\mathbb{R}^{\n",
    "        [\\mathrm{out}]\n",
    "        \\times [\\mathrm{in}]\n",
    "        \\times h\n",
    "        \\times w}\n",
    "$ relative to $(i, j)$ coordinates in $y$.\n",
    "The exact values of $i_p$ and $j_q$ depend on the configuration of the\n",
    "convolutional layer, e.g. stride, kernel size and dilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Implementing `DropoutConv2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images we don't usually use the `F.dropout` form our previous\n",
    "experiments, because when applied to the data $\n",
    "x \\in \\mathbb{R}^{\n",
    "    [\\mathrm{in}]\n",
    "    \\times h\n",
    "    \\times w\n",
    "}\n",
    "$ it would affect random pixels within each input feature: \n",
    "\n",
    "$$\n",
    "\\mathrm{F.dropout}(x)\n",
    "    \\colon x \\mapsto x \\odot m\n",
    "    = \\bigl( x_{kij} \\, m_{kij} \\bigr)_{kij}\n",
    "    \\,. $$\n",
    "\n",
    "Please use `F.dropout2d` instead. You would also need to invoke `F.conv2d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)**\n",
    "* to view documentation on something  type in `something?` (with one question mark)\n",
    "* to view code of something type in `something??` (with two question marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutConv2d(Conv2d, FreezableWeight):\n",
    "    \"\"\"2d Convolutional layer with dropout on input features.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros',\n",
    "                 p=0.5):\n",
    "\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                         padding=padding, dilation=dilation, groups=groups,\n",
    "                         bias=bias, padding_mode=padding_mode)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):        \n",
    "        \"\"\"Apply feature dropout and then forward pass through the convolution.\"\"\"\n",
    "        # Exercise: write a forward pass similar to `DropoutLinear.forward`,\n",
    "        #  but first, take a look at the code `Conv2d.forward` in a new cell.\n",
    "        #  It will help you understand what to pass to `F.conv2d`. For cleaner\n",
    "        #  code you may use a super-method.\n",
    "\n",
    "        # linear with frozen weight\n",
    "        if self.is_frozen():\n",
    "            return F.conv2d(input, self.frozen_weight, self.bias, self.stride,\n",
    "                            self.padding, self.dilation, self.groups)\n",
    "\n",
    "        return super().forward(F.dropout2d(input, self.p, True))\n",
    "\n",
    "        pass\n",
    "\n",
    "    def freeze(self):\n",
    "        \"\"\"Sample the weight from the parameter distribution and freeze it.\"\"\"\n",
    "        ## Exercise: much like in `DropoutLinear.freeze` dropout input\n",
    "        #  filters in the convolutional kernel.\n",
    "\n",
    "        prob = torch.full_like(self.weight[:1, :, :1, :1], 1 - self.p)\n",
    "        feature_mask = torch.bernoulli(prob) / prob\n",
    "\n",
    "        frozen_weight = self.weight * feature_mask\n",
    "\n",
    "        self.register_buffer(\"frozen_weight\", frozen_weight)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** For more on convolutions see\n",
    "[Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic) \n",
    "repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the `SimpleModel` class above in $1$d section,\n",
    "let's implement a simple deep convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    \"\"\"A simple convolutional net.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = torch.nn.Sequential(\n",
    "            Conv2d(1, 20, 5, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(2),\n",
    "\n",
    "            DropoutConv2d(20, 50, 5, 1, p=p),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.AvgPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = DropoutLinear(4 * 4 * 50, 400, p=p)\n",
    "        self.out = DropoutLinear(400, 10, p=p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Take images and compute their class logits.\"\"\"\n",
    "        x = self.conv_block(input).flatten(1)\n",
    "        return self.out(F.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the classifier that outputs raw logit scores, we typically\n",
    "normalize outputs via `F.log_softmax` and then feed into them into\n",
    "`F.nll_loss`, which computes the negative $\\log$-likelihood of a\n",
    "categorical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is less numerically stable then using `F.cross_entropy`,\n",
    "which is essentially` log_softmax + nll` fused into one stable operation.\n",
    "It is good practice to pay attention to numerical stability, especially\n",
    "when working with `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(model, X, y):\n",
    "    return F.cross_entropy(model(X), y)  # + coef * sum(penalties(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actively Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will partially replicate figure 1. in [Gat et al. (2017): p. 4](http://proceedings.mlr.press/v70/gal17a.html),\n",
    "\n",
    "> Gal, Y., Islam, R. & Ghahramani, Z.. (2017). Deep Bayesian Active Learning with Image Data. Proceedings of the 34th International Conference on Machine Learning, in [PMLR 70:1183-1192](http://proceedings.mlr.press/v70/gal17a.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the datasets from the `train` part of [MNIST](http://yann.lecun.com/exdb/mnist/):\n",
    "* ($\\mathcal{S}_\\mathrm{train}$) initial **training**:\n",
    "  **empty** -- learn from scratch\n",
    "  <strike>21 images, purposefully highly imbalanced classes (even absent ones)</strike>\n",
    "* ($\\mathcal{S}_\\mathrm{valid}$) our **validation**:\n",
    "  $5000$ images, stratified\n",
    "* ($\\mathcal{S}_\\mathrm{pool}$) acquisition **pool**:\n",
    "  all remaining images\n",
    "\n",
    "The true test sample of MNIST is in $\\mathcal{S}_\\mathrm{test}$ -- we\n",
    "will use it to evaluate the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import get_dataset\n",
    "\n",
    "S_train, S_pool, S_valid, S_test = get_dataset(\n",
    "    n_train=0, n_valid=5000, name=\"MNIST\",\n",
    "    random_state=722_257_201, path=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(note)** We may just as well use [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to plot images in a small dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.flex import plot\n",
    "\n",
    "def display(dataset, title=None, show_balance=True, figsize=None):\n",
    "    images, targets = dataset.tensors\n",
    "    if not show_balance:\n",
    "        balance = \"\"\n",
    "\n",
    "    else:\n",
    "        body = [f\"{n:2d}\" if n > 0 else \" *\"\n",
    "                for n in label_counts(targets)]\n",
    "        balance = \"(freq) [ \" + ' '.join(body) + \" ]\"\n",
    "\n",
    "    # a canvas\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    # show the images\n",
    "    plot(ax, images, cmap=plt.cm.bone)\n",
    "\n",
    "    # produce a title\n",
    "    title = \"\" if title is None else title\n",
    "    title = title + (\" \" if title else \"\")\n",
    "    ax.set_title(f\"{title}{balance}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def label_counts(labels, n_labels=10):\n",
    "    return np.bincount(labels.numpy(), minlength=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(S_train, title=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary components for the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to manipulate the datasets for the **main active learning**\n",
    "loop. We begin by implementing the following primitives:\n",
    "* `take` collect the instances at the specified indices into a **new dataset** (object)\n",
    "* `append` add one dataset to another\n",
    "* `delete` drops the instances at the specified locations form the copy of the **dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlss2019bdl.dataset import take, delete, append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **main active learning** loop, besides manipulating the datasets,\n",
    "we shall also need a function to **predict and acquire** and evaluate\n",
    "holdout **performance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, dataset, n_samples=1):\n",
    "    logits = sample_function(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    # logit-scores should be transformed into a proper distribution\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire(model, dataset, n_points=10, n_samples=1):\n",
    "    proba = predict_proba(model, dataset, n_samples=n_samples)\n",
    "\n",
    "    _, _, mutual_info = mutual_information(proba)\n",
    "\n",
    "    return acq_bald(mutual_info, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, n_samples=1):\n",
    "    proba = predict_proba(model, dataset, n_samples=n_samples)\n",
    "    \n",
    "    proba_avg = proba.mean(dim=0)\n",
    "\n",
    "    predicted = proba_avg.argmax(dim=-1).numpy()\n",
    "    target = dataset.tensors[1].cpu().numpy()\n",
    "\n",
    "    return confusion_matrix(target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (task) Implementing the active learning step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code the core of the active learning loop:\n",
    "\n",
    "1. fit on **train**, then (optional) evaluate on **holdout**\n",
    "2. acquire from **pool**\n",
    "3. add to **train** (removing from **pool**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_step(model, S_train, S_pool,\n",
    "                         n_epochs=5, n_points=10, n_samples=11):\n",
    "    ## Exercise: implement the fit-acquire loop\n",
    "\n",
    "    # 1. fit on S_train using `cross_entropy`, set `weight_decay` to 1e-4\n",
    "    fit(model, S_train, criterion=cross_entropy, n_epochs=n_epochs, weight_decay=1e-4)\n",
    "\n",
    "    # 2. acquire new instances from S_pool\n",
    "    indices = acquire(model, S_pool, n_points=n_points, n_samples=n_samples)\n",
    "\n",
    "    # 3. query the pool for the chosen instances, then take-append-delete\n",
    "    S_requested = take(S_pool, indices)\n",
    "    S_train = append(S_train, S_requested)\n",
    "    S_pool = delete(S_pool, indices)\n",
    "\n",
    "    pass\n",
    "\n",
    "    return model, S_train, S_pool, S_requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNNModel(p=0.5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we Recall that it consists of the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs, n_samples = 5, 11\n",
    "n_active, n_points = 75, 10\n",
    "\n",
    "display(S_train, title=\"initial train\")\n",
    "\n",
    "scores = []\n",
    "balances = [label_counts(S_train.tensors[1])]\n",
    "for step in range(n_active):\n",
    "    model, S_train, S_pool, S_requested = active_learning_step(\n",
    "        model, S_train, S_pool, n_epochs=n_epochs,\n",
    "        n_points=n_points, n_samples=n_samples)\n",
    "\n",
    "    # (optional) track validation score\n",
    "    score_matrix = evaluate(model, S_valid, n_samples=n_samples)\n",
    "\n",
    "    # (optional) report accuracy and the statistics on the acquired batch\n",
    "    balances.append(label_counts(S_train.tensors[1]))\n",
    "    scores.append(score_matrix)\n",
    "\n",
    "    accuracy = score_matrix.diagonal().sum() / score_matrix.sum()\n",
    "    display(S_requested, title=f\"# {len(S_train)} (Acc.) {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train of the final $\\mathcal{S}_\\mathrm{train}$ and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balances = np.stack(balances, axis=0)\n",
    "\n",
    "fit(model, S_train, criterion=cross_entropy, n_epochs=n_epochs, weight_decay=1e-4)\n",
    "scores.append(evaluate(model, S_valid, n_samples=n_samples))\n",
    "\n",
    "scores = np.stack(scores, axis=0)\n",
    "\n",
    "display(S_train, title=\"final train\", figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the dynamics of the frequency of each class in $\\mathcal{S}_\\mathrm{train}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(balances, lw=2)\n",
    "plt.legend(lines, list(range(10)), ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics of *one-versus-rest* precision / recall scores on\n",
    "$\\mathcal{S}_\\mathrm{valid}$. For binary classification:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\mathrm{Precision}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\n",
    "        \\approx \\mathbb{P}(y = 1 \\mid \\hat{y} = 1)\n",
    "    \\,, \\\\\n",
    "\\mathrm{Recall}\n",
    "    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}\n",
    "        \\approx \\mathbb{P}(\\hat{y} = 1 \\mid y = 1)\n",
    "    \\,.\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = scores.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = scores.sum(axis=-2) - tp, scores.sum(axis=-1) - tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(tp / (tp + fp), lw=2)\n",
    "ax.set_title(\"Precision (ovr)\")\n",
    "ax.legend(lines, list(range(10)), ncol=2);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "lines = ax.plot(tp / (tp + fn), lw=2)\n",
    "ax.set_title(\"Recall (ovr)\")\n",
    "ax.legend(lines, list(range(10)), ncol=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy as a function of active learning iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n",
    "\n",
    "ax.plot(tp.sum(-1) / scores.sum((-2, -1)),\n",
    "        label='Accuracy', lw=2)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volume of data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"train : pool = {len(S_train)} : {len(S_pool)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the test set confusion matrix be the ultimate judge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_matrix = evaluate(model, S_test, n_samples=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives, and false positives / negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = score_matrix.diagonal(axis1=-2, axis2=-1)\n",
    "fp, fn = score_matrix.sum(axis=-2) - tp, score_matrix.sum(axis=-1) - tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"(accuracy) {tp.sum() / score_matrix.sum():.2%}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-v-rest precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fp))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ovr recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{l: f\"{p:.2%}\" for l, p in enumerate(tp / (tp + fn))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = predict_proba(model, S_test, n_samples=21)\n",
    "proba_avg, ent, mi = mutual_information(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "indices = entj.argsort().numpy()[:256]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "plot_dataset(TensorDataset(*S_test[indices]), n_images=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ent_n, mi_n = mutual_information(proba, jackknife=False)\n",
    "_, ent_j, mi_j = mutual_information(proba, jackknife=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mi_n.numpy(), mi_n.numpy()-mi_j.numpy(), s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersting stuff, that didn't make the cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense network for MNIST, in case convolutions are too slow on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(torch.nn.Module):\n",
    "    \"\"\"A fully connected net.\"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        self.body = torch.nn.Sequential(\n",
    "            Linear(784, 256),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            DropoutLinear(256, 256, p=p),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            DropoutLinear(256, 256, p=p),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            DropoutLinear(256, 10, p=p),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.body(input.flatten(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key issue with point-estimates is that values at each $x$ jointly do not\n",
    "correspond to a function in the parametric family modelled by the network:\n",
    "there may be no $\\omega \\in \\mathop{supp}q_\\theta(\\omega)$ such that\n",
    "$f_\\omega(x_i) = y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational approximation via Gaussian mean field family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sampling from $q_\\theta(\\omega) = q_\\theta(b) \\otimes q_\\theta(W)$ where\n",
    "$q_\\theta(b) = \\delta_{b - \\beta}$, $q_\\theta(W) = \\otimes_{ij} q_\\theta(W_{ij})$\n",
    "and $\n",
    "q_\\theta(W_{ij})\n",
    "    = \\mathcal{N}\\bigl(\n",
    "        W_{ij} \\big\\vert\n",
    "            \\mu_{ij}, \\sigma^2_{ij}\n",
    "        \\bigr)\n",
    "$;\n",
    "* **local** reparameterization trick $y = x W + b$ implies that $\n",
    "    y_j \\sim \\mathcal{N}\\bigl(\n",
    "            \\beta_j + \\sum_i x_i \\mu_{ij},\n",
    "            \\sum_i \\sigma^2_{ij} \\lvert x_i \\rvert^2\n",
    "        \\bigr)\n",
    "$ and $y_j \\bot y_k$, $j\\neq k$;\n",
    "* computing the KL-divergence of $q_\\theta(W)$ from $p(W)$ ignoring $b$: $\n",
    "    \\mathop{KL}\\bigl(q_\\theta(W) \\| p(W)\\bigr)\n",
    "        = \\mathbb{E}_{W \\sim q_\\theta}\n",
    "            \\log \\tfrac{q_\\theta(W)}{p(W)}\n",
    "$\n",
    "  * objective diffuse prior $p(W_{ij}) \\propto \\mathop{const}$\n",
    "  * objective scale-free prior $p(W_{ij}) \\propto \\tfrac1{\\lvert W_{ij} \\rvert}$\n",
    "  * subjective prior $p(W_{ij}) = \\mathcal{N}(W_{ij} \\mid 0, \\nu^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the this to work, it is necessary to implement\n",
    "another **trait-class** and a penalty \"collector\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariatonalApproximation(FreezableWeight):\n",
    "    def penalty(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalties(module):\n",
    "    for mod in module.modules():\n",
    "        if isinstance(mod, VariatonalApproximation):\n",
    "            yield mod.penalty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffuse prior $p(W_{ij}) \\propto \\mathop{const}$\n",
    "\n",
    "Against a **diffuse prior** the KL-divergence is just\n",
    "the **negative entropy** (up to a constant).\n",
    "\n",
    "Since $q_\\theta(W)$ is a diagonal multivariate normal\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_{ij} KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "        = \\mathop{const} - \\sum_{ij} \\mathbb{H}(q_\\theta(W_{ij}))\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multivariate Gaussain $\n",
    "    p(z) = \\mathcal{N}_n\\bigl(\n",
    "        z\\,\\big\\vert\\, \\mu, \\Sigma\n",
    "    \\bigr)\n",
    "$ we have\n",
    "$$\n",
    "    \\mathbb{H}(p)\n",
    "        = - \\mathbb{E}_{z\\sim p} \\log p(z)\n",
    "        = \\tfrac12 \\log \\det \\bigl(2 \\pi e \\Sigma \\bigr)\n",
    "%         = \\tfrac12 \\log \\det \\Sigma + \\tfrac{n}2 \\log 2 \\pi e\n",
    "    \\,. $$\n",
    "\n",
    "Hence the entropy for a univariate Gaussian is\n",
    "$$\n",
    "    \\mathbb{H}(q_\\theta(W_{ij}))\n",
    "        = - \\mathbb{E}_{W_{ij} \\sim q_\\theta(W_{ij})} \\log q_\\theta(W_{ij})\n",
    "        = \\tfrac12 \\log \\{2 \\pi e \\, \\sigma^2\\}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_diffuse(log_sigma2, weight=None):\n",
    "\n",
    "    const = 0.5 * math.log(2 * math.pi * math.e) * log_sigma2.numel()\n",
    "    entropy = const + 0.5 * torch.sum(log_sigma2)\n",
    "\n",
    "    return - entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper prior $p(W_{ij}) = \\mathcal{N}(W_{ij} \\mid 0, \\nu^{-1})$\n",
    "\n",
    "The KL-divergence between two multivariate Gaussians is given by\n",
    "\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\mathcal{N}_m(\\mu_0, \\Sigma_0)\n",
    "        \\big\\| \\mathcal{N}_m(\\mu_1, \\Sigma_1)\n",
    "    \\bigr)\n",
    "        = \\frac12 \\Bigl\\{\n",
    "            \\log \\frac{\\det \\Sigma_1}{\\det \\Sigma_0}\n",
    "            + \\mathop{tr} \\bigl( \\Sigma_1^{-1} \\Sigma_0 \\bigr)\n",
    "            + \\bigl(\\mu_0 - \\mu_1 \\bigr)^\\top \\Sigma_1^{-1} \\bigl(\\mu_0 - \\mu_1 \\bigr)\n",
    "            - m\n",
    "        \\Bigr\\}\n",
    "    \\,. $$\n",
    "\n",
    "KL divergence from a standard normal distribution is\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\mathcal{N}_m(\\mu_0, \\Sigma_0)\n",
    "        \\big\\| \\mathcal{N}_m(0, \\nu^{-1} I_m)\n",
    "    \\bigr)\n",
    "        = \\frac12 \\Bigl\\{\n",
    "            \\nu \\, (\\mathop{tr} \\Sigma_0 + \\mu_0^\\top \\mu_0)\n",
    "            - m \\log \\nu\n",
    "            - \\log \\det \\Sigma_0\n",
    "            - m\n",
    "        \\Bigr\\}\n",
    "    \\,. $$\n",
    "<!--\n",
    "$$\n",
    "    \\frac12 \\Bigl\\{\n",
    "        - \\log \\det \\nu \\Sigma_0\n",
    "        + \\nu \\mathop{tr} \\bigl( \\Sigma_0 \\bigr)\n",
    "        + \\nu \\bigl(\\mu_0 - \\mu_1 \\bigr)^\\top \\bigl(\\mu_0 - \\mu_1 \\bigr)\n",
    "        - m\n",
    "    \\Bigr\\}\n",
    "    \\,. $$\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, if $q_\\theta(W)$ is a diagonal multivariate normal then we get\n",
    "(put $m=1$, $\\Sigma_0 = \\sigma^2_{ij}$, $\\mu_1 = 0$):\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_{ij} KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "        = \\frac12 \\sum_{ij} \\bigl(\n",
    "            \\nu \\sigma^2_{ij} + \\nu \\mu_{ij}^2\n",
    "            - \\log \\sigma^2_{ij} - \\log \\nu - 1\n",
    "        \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_proper(log_sigma2, weight, nu=1.0):\n",
    "\n",
    "    const = 0.5 * (math.log(nu) + 1) * log_sigma2.numel()\n",
    "    nu_term = 0.5 * nu * (torch.exp(log_sigma2) + weight * weight)\n",
    "    kl_div = torch.sum(nu_term - log_sigma2) - const\n",
    "\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_1811_00596(log_sigma2, weight):\n",
    "    r\"\"\"Penalty from arxiv:1811.00596.\n",
    "\n",
    "    The precision parameter `\\nu` in the prior is optimized away.\n",
    "    \"\"\"\n",
    "    # get $- \\log \\alpha_{ij}$\n",
    "    neg_log_alpha = 2 * torch.log(abs(weight) + 1e-12) - log_sigma2\n",
    "\n",
    "    # `softplus` is $x \\mapsto \\log(1 + e^x)$\n",
    "    kl_div_approx = torch.sum(0.5 * F.softplus(neg_log_alpha))\n",
    "\n",
    "    return kl_div_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale-free prior $p(W_{ij}) \\propto \\tfrac1{\\lvert W_{ij} \\rvert}$\n",
    "\n",
    "This prior gives us the so called Variational Dropout\n",
    "([Molchanov et al. 2017](https://arxiv.org/abs/1701.05369),\n",
    "[Kingma et al. 2015](https://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick)).\n",
    "\n",
    "We may observe the following, unless $\\mu_{ij} = 0$:\n",
    "for $\\alpha_{ij} = \\tfrac{\\sigma^2_{ij}}{\\mu_{ij}^2}$\n",
    "\n",
    "$$\n",
    "    \\mathcal{N}(\\mu_{ij}, \\sigma^2_{ij})\n",
    "    \\overset{D}{\\sim} \\mu_{ij} \\cdot \\mathcal{N}(1, \\alpha_{ij})\n",
    "    \\,. $$\n",
    "\n",
    "Therefore our variational approximation $q_\\theta(W)$ can be regarded as\n",
    "the so called Gaussian Dropout (): $W_{ij}$ are subject to multiplicative\n",
    "noise $W_{ij} = \\mu_{ij} \\cdot \\varepsilon_{ij}$ for $\\varepsilon_{ij}\n",
    "\\sim \\mathcal{N}(1, \\alpha_{ij})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this variational family:\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "%         = \\mathop{const} - \\tfrac12 \\log \\sigma^2_{ij}\n",
    "%         + \\mathbb{E}_{W_{ij} \\sim q_\\theta(W_{ij})} \\log \\lvert W_{ij} \\rvert\n",
    "        = \\mathop{const} - \\tfrac12 \\log \\sigma^2_{ij}\n",
    "        + \\tfrac12 \\log \\mu_{ij}^2\n",
    "        + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(0, 1)}\n",
    "            \\log \\lvert 1 + \\tfrac{\\sigma_{ij}}{\\lvert \\mu_{ij} \\rvert} \\xi \\rvert\n",
    "    \\,. $$\n",
    "\n",
    "Unfortunately there is no closed-from expression for this divergence, and thus\n",
    "we have to resort to its approximation in ICML'17 paper [Molchanov et al. 2017](https://arxiv.org/abs/1701.05369):\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W_{ij}) \\big\\| p(W_{ij}) \\bigr)\n",
    "%         = \\mathop{const}\n",
    "%         + \\mathbb{E}_{\\xi \\sim \\mathcal{N}(1, \\alpha_{ij})}\n",
    "%                         \\log{\\lvert \\xi \\rvert}\n",
    "%                     - \\tfrac12 \\log \\alpha_{ij}\n",
    "        \\approx\n",
    "            \\tfrac12 \\log (1 + e^{-\\log \\alpha}) \n",
    "            + k_1(1 - \\sigma(k_2 + k_3 \\log \\alpha))\n",
    "    \\,, $$\n",
    "\n",
    "where $k_1, k_2, k_3$ and $C$ are approximated by $0.63576$, $1.87320$ and $1.48695$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_scale_free(log_sigma2, weight):\n",
    "\n",
    "    # get $- \\log \\alpha_{ij}$\n",
    "    neg_log_alpha = 2 * torch.log(abs(weight) + 1e-12) - log_sigma2\n",
    "    \n",
    "    # Use the identity 1 - \\sigma(z) = \\sigma(- z)\n",
    "    sigmoid = torch.sigmoid(1.48695 * neg_log_alpha - 1.87320)\n",
    "\n",
    "    # `softplus` is $x \\mapsto \\log(1 + e^x)$\n",
    "    kl_div_approx_ij = 0.5 * F.softplus(neg_log_alpha) + 0.63576 * sigmoid\n",
    "    kl_div_approx = torch.sum(kl_div_approx_ij)\n",
    "\n",
    "    return kl_div_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The local reparameterization tirck\n",
    "\n",
    "Proposed in\n",
    "[Kingma et al. 2015](https://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick)\n",
    "this tirck\n",
    "* allows to sample parameters implicitly\n",
    "* reduces variance of the stochastic gradient, generally leading to much faster convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of a linear layer on its input is given by the equation\n",
    "$$\n",
    "    y = x W + b\n",
    "    \\,, \\tag{obvious} $$\n",
    "with $W \\in \\mathbb{R}^{m\\times n}$, $y\\in \\mathbb{R}^n$,\n",
    "$x\\in \\mathbb{R}^m$ and $b \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observation:\n",
    "> any non-trivial linear combination of Gaussian\n",
    "random variables is a Gaussian random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the layer's $W$ are jointly Gaussian, the means that the distribution\n",
    "of $y$ is also Gaussian. We can see this if we look closely at the output: the bias term is effectively\n",
    "fixed to $\\beta$ and the weights are\n",
    "\n",
    "$$\n",
    "    W_{ij} \\sim\n",
    "        \\mathcal{N}(M_{ij}, \\sigma^2_{ij})\n",
    "    \\,,\\, W_{ij} \\bot W_{kl}\n",
    "    \\,,\\, ij\\neq kl\n",
    "    \\,,\n",
    "$$\n",
    "\n",
    "which means that\n",
    "$$\n",
    "    y_j = \\beta_j + \\sum_i x_i W_{ij}\n",
    "    \\Rightarrow\n",
    "    y_j \\sim \\mathcal{N}\\bigl(\n",
    "        \\beta_j + \\sum_i x_i M_{ij}, \\sum_i \\sigma^2_{ij} x_i^2\n",
    "    \\bigr)\n",
    "    \\tag{local-reparametrization}\n",
    "    \\,. $$\n",
    "\n",
    "Note that independence of $W_{ij}$ implies zero correlation between them, which\n",
    "means that distinct $y_j$ and $y_k$ are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting into a single multivariate Gaussian random vector we get the \n",
    "following stochastic forward pass:\n",
    "\n",
    "$$\n",
    "    y \\sim \\mathcal{N}_n\\bigl(\n",
    "        x M + \\beta\\,,\\, \\mathrm{diag}(\\nu^2)\n",
    "    \\bigr)\n",
    "    \\,, $$\n",
    "\n",
    "with $\\nu^2_j = \\sum_{i=1}^m \\sigma^2_{ij} x_i^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_linear_lrp(layer, input):\n",
    "    \"\"\"Forward pass for the linear layer with the local reparameterization trick.\"\"\"\n",
    "\n",
    "    ## Exercise: implement the always active local reparametrization trick.\n",
    "    #  (note) you might want to add 1e-20 inside `.sqrt()`\n",
    "\n",
    "    # Get the mean\n",
    "    mu = F.linear(input, layer.weight, layer.bias)\n",
    "\n",
    "    # Add the resulting effect of weight randomness\n",
    "    s2 = F.linear(input * input, torch.exp(layer.log_sigma2), None)\n",
    "    output =  mu + torch.randn_like(s2) * torch.sqrt(s2 + 1e-20)\n",
    "\n",
    "    pass\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer with Gaussian dropout and the trick "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the trick for the Gaussian approximation $q_\\theta(\\omega)$\n",
    "and one of the divergences, given above, into a stochastic linear layer:\n",
    "\n",
    "* we need write the forward pass and a parameter sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLinear(Linear, VariatonalApproximation):\n",
    "    \"\"\"Linear layer with Gaussian Mean Field weight distribution.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.log_sigma2 = torch.nn.Parameter(\n",
    "            torch.Tensor(*self.weight.shape))\n",
    "\n",
    "        self.reset_variational_parameters()\n",
    "\n",
    "    def reset_variational_parameters(self):\n",
    "        \"\"\"Initialize the log-variance.\"\"\"\n",
    "        self.log_sigma2.data.normal_(-8, 0.01)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass with the local reparameterization trick.\"\"\"\n",
    "        if self.is_frozen():\n",
    "            return F.linear(input, self.frozen_weight, self.bias)\n",
    "\n",
    "        return stochastic_linear_lrp(self, input)\n",
    "\n",
    "    def freeze(self):\n",
    "        \"\"\"Return a sample from $q_{\\theta_m}(\\omega_m)$.\"\"\"\n",
    "        \n",
    "        ## Exercise: sample the weights from the variational approximation\n",
    "        stdev = torch.exp(0.5 * self.log_sigma2)\n",
    "        weight = torch.normal(self.weight, std=stdev)\n",
    "\n",
    "        self.register_buffer(\"frozen_weight\", weight)\n",
    "\n",
    "    @property\n",
    "    def penalty(self):\n",
    "        \"\"\"KL divergence between $q_{\\theta_m}(\\omega_m)$ an a prior on $\\omega_m$.\"\"\"\n",
    "\n",
    "        ## Exercise\n",
    "        # return kl_div_diffuse(self.log_sigma2, self.weight)\n",
    "        # return kl_div_scale_free(self.log_sigma2, self.weight)\n",
    "        # return kl_div_proper(self.log_sigma2, self.weight, nu=1e0)\n",
    "\n",
    "        return kl_div_1811_00596(self.log_sigma2, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution can be represented as matrix-vector product of the doubly\n",
    "block-circulant embedding (Toeplitz) of the kernel and the unravelled\n",
    "input. As such, it is an implicit linear layer with block structured\n",
    "weight matrix, but unlike it, the local reparameterization trick has\n",
    "a little caveat. If the kernel itself is assumed to have the specified\n",
    "variational distribution, then the outputs will be spatially correlated\n",
    "due to the same weight block being reused at each location:\n",
    "\n",
    "$$\n",
    "    cov(y_{f\\beta}, y_{k\\omega})\n",
    "        = \\delta_{f=k} \\sum_{c \\alpha}\n",
    "            \\sigma^2_{fc \\alpha}\n",
    "            x_{c i_\\beta(\\alpha)}\n",
    "            x_{c i_\\omega(\\alpha)}\n",
    "    \\,, $$\n",
    "\n",
    "where $i_\\beta(\\alpha)$ is the location in $x$ for the output location\n",
    "$\\beta$ and kernel offset $\\alpha$ (depends on stride and dilation).\n",
    "\n",
    "In contrast, if instead the Toeplitz embedding blocks are assumed iid\n",
    "draws from the variational distribution, then covariance becomes\n",
    "\n",
    "$$\n",
    "    cov(y_{f\\beta}, y_{k\\omega})\n",
    "        = \\delta_{f\\beta = k\\omega} \\sum_{c \\alpha}\n",
    "            \\sigma^2_{fc \\alpha}\n",
    "            \\lvert x_{c i_\\omega(\\alpha)} \\rvert^2\n",
    "    \\,. $$\n",
    "\n",
    "Molchanov et al. (2017) implicitly assume that kernels is are iid draws\n",
    "from the variational distribution for different spatial locations. This\n",
    "effectively zeroes the spatial cross-correlation in the output, reduces\n",
    "the variance of the gradient in SGVB method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConv2d(Conv2d, VariatonalApproximation):\n",
    "    \"\"\"Convolutional layer with Gaussian Mean Field weight distribution.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True, p=0.05,\n",
    "                 padding_mode='zeros'):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                         padding=padding, dilation=dilation, groups=groups,\n",
    "                         bias=bias, padding_mode=padding_mode)\n",
    "\n",
    "        self.log_sigma2 = torch.nn.Parameter(\n",
    "            torch.Tensor(*self.weight.shape))\n",
    "\n",
    "        self.reset_variational_parameters()\n",
    "\n",
    "    reset_variational_parameters = LinearGaussian.reset_variational_parameters\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass with the local reparameterization trick.\"\"\"\n",
    "        if self.is_frozen():\n",
    "            return F.conv2d(input, self.frozen_weight, None,\n",
    "                            self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        mu = super().forward(input)\n",
    "\n",
    "        s2 = F.conv2d(input * input, torch.exp(self.log_sigma2), None,\n",
    "                      self.stride, self.padding, self.dilation, self.groups)\n",
    "        return mu + torch.randn_like(s2) * torch.sqrt(s2 + 1e-20)\n",
    "\n",
    "    # we reuse implementation, without inheritance\n",
    "    freeze = GaussianLinear.freeze\n",
    "\n",
    "    penalty = GaussianLinear.penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer with Bernoulli Dropout and Gaussian approximation\n",
    "\n",
    "We can also fuse the classical Bernoulli dropout, [Hinton et al. 2012](https://arxiv.org/abs/1207.0580)\n",
    "with Gaussian variational approximation [Gal, Y. 2016](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of the fused model is simply a composition of dropout\n",
    "on inputs and the local reparameterization trick. However, for the Kullback-Leibler\n",
    "divergence we need to identify how $W$ are effectively distributed. Indeed,\n",
    "the variational approximation $q_{\\theta}(W)$ is essentially a spike-and-slab\n",
    "mixture: each row $W_i$ of $W\\in \\mathbb{R}^{m \\times n}$ is either $\\mathbf{0}$\n",
    "with probability $p$ or a Gaussian vector in $\\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "    W_i \\sim\n",
    "\\begin{cases}\n",
    "    \\mathbf{0}\n",
    "        & \\text{ w. prob } p \\\\\n",
    "    M_i\n",
    "        & \\text{ otherwise } \\\\\n",
    "\\end{cases}\n",
    "    \\,, \\text{indep.}\n",
    "    \\,, i=1, \\ldots, m\n",
    "    \\,, M_{ij} \\sim \\mathcal{N}_n\\bigl(\n",
    "        M_{ij} \\big\\vert \\mu_i, \\mathop{diag} \\sigma^2_i\n",
    "    \\bigr)\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under some assumptions and benign relaxations\n",
    "[Gal, Y. 2016 (eq. (6.3) p.109, Prop. 4 p.149)](http://www.cs.ox.ac.uk/people/yarin.gal/website/thesis/thesis.pdf)\n",
    "the divergence can approximated by\n",
    "\n",
    "$$\n",
    "    KL\\bigl( q_\\theta(W) \\big\\| p(W) \\bigr)\n",
    "        = \\sum_i KL\\bigl( q_\\theta(W_i) \\big\\| p(W_i) \\bigr)\n",
    "        \\approx \\mathop{const}\n",
    "        % + \\frac{mn} 2 p \\{\\tau^{-1} + \\log \\tau\\}\n",
    "        % + m (p \\log p + (1-p) \\log (1-p))\n",
    "        + \\frac{1-p}2 \\sum_{ij}\n",
    "            \\sigma^2_{ij} + \\mu_{ij}^2 - \\log \\sigma^2_{ij}\n",
    "    \\,. $$\n",
    "\n",
    "<!--\n",
    "Proposition 4 p.149 is about the approximation of the divergence of\n",
    "a mixture from the standard Gaussian:\n",
    "\n",
    "$$\n",
    "    KL\\bigl(\n",
    "        \\sum_k \\pi_k \\mathcal{N}_n(\\mu_k, \\Sigma_k)\n",
    "        \\big\\| \\mathcal{N}_n(0, I_n)\n",
    "    \\bigr)\n",
    "        \\approx \\mathop{const}\n",
    "        + \\frac12 \\sum_k \\pi_k \\{\n",
    "            \\mathop{tr}\\Sigma_k\n",
    "            + \\mu_k^\\top \\mu_k\n",
    "            - \\log\\det\\Sigma_k\n",
    "        \\}\n",
    "        - \\mathbb{H}(\\pi)\n",
    "    \\,. $$\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFusedGaussianBernoulli(LinearGaussian):\n",
    "    \"\"\"Linear layer with spike-slab (Gaussian) weight distribution.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, p=0.5):\n",
    "        super().__init__(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward pass with Bernoulli dropout, then the Gaussian \n",
    "        local reparameterization trick.\n",
    "        \"\"\"\n",
    "        input = F.dropout(input, self.p, True)\n",
    "\n",
    "        return super().forward(input)\n",
    "\n",
    "    def sample(self):\n",
    "        par = super().sample()\n",
    "\n",
    "        prob = torch.full_like(self.weight[:1], 1 - self.p)\n",
    "        par[\"weight\"] *= torch.bernoulli(prob) / prob\n",
    "\n",
    "        return par\n",
    "\n",
    "    @property\n",
    "    def penalty(self):\n",
    "        \"\"\"Approximate KL divergence.\"\"\"\n",
    "        return (1 - self.p) * kl_div_proper(self.log_sigma2, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubles with initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a matrix $W \\in\\mathbb{R}^{\n",
    "[\\mathrm{out}]\n",
    "\\times [\\mathrm{in}]\n",
    "}$. Out and in are known as `fan-in` and `fan-out` respectively.\n",
    "\n",
    "Kaiming He:\n",
    "* (normal) $w_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\n",
    "    \\sigma = \\tfrac{\\sqrt2}{\\sqrt{\n",
    "        [\\mathrm{in}] (1+a^2)\n",
    "    }}\n",
    "$ ($a$ -- negative slope of the ReLU)\n",
    "* (uniform) $w_{ij} \\sim \\mathcal{U}[-\\sigma, +\\sigma]$ with $\n",
    "\\sigma = \\tfrac{\\sqrt6}{\\sqrt{\n",
    "        [\\mathrm{in}] (1+a^2)\n",
    "}}$\n",
    "\n",
    "Glorot Xavier:\n",
    "* (uniform) $w_{ij} \\sim \\mathcal{U}[-b, +b]$ with $\n",
    "b = \\mathrm{gain} \\cdot \\sqrt{\\tfrac6{\n",
    "        [\\mathrm{out}] + [\\mathrm{in}]\n",
    "    }}\n",
    "$\n",
    "* (normal) $w_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$ with $\n",
    "\\sigma = \\mathrm{gain} \\cdot \\sqrt{\\tfrac2{\n",
    "        [\\mathrm{out}] + [\\mathrm{in}]\n",
    "    }}\n",
    "$\n",
    "\n",
    "\n",
    "Based on this piece on [weight initialization](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "from torch.nn import Linear\n",
    "\n",
    "class Linear(torch.nn.Linear):\n",
    "    def reset_parameters(self):\n",
    "        init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            init.zeros_(self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about heteroskedasticity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heteroskedastic_mse_loss(model, X, y):\n",
    "    output = model(X)\n",
    "    mean, log_sigma = output[..., [0]], output[..., [1]]\n",
    "\n",
    "    value = F.mse_loss(mean, y, reduction=\"none\")\n",
    "\n",
    "    return torch.mean(value * torch.exp(- log_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $1$-d classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_loss(model, X, y):\n",
    "    return F.binary_cross_entropy_with_logits(model(X), y)\n",
    "\n",
    "fit(model, train, criterion=log_loss, batch_size=32, n_epochs=2000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting induces\n",
    "a distribution of the observed data $D$ conditional on the model\n",
    "$f$ and its parameters $\\omega$: $p(y \\mid x, \\omega, f)$.\n",
    "\n",
    "parameters to be optimized over, but rather as random variables the\n",
    "distribution of which **after** observing the data is to be sought.\n",
    "\n",
    "* a prior distribution $p(\\omega)$ on the parameters $\\omega$ -- this\n",
    "  reflects our belief prior to observing data.\n",
    "\n",
    "* $p(y \\mid x, \\omega)$ -- the output distribution (Gaussian, Categorical,\n",
    "  Poisson), the parameters of which are modeled by some function of the\n",
    "  input $f_\\omega\\colon \\mathcal{X} \\to \\mathcal{Y}$. We construct the\n",
    "  **likelihood** with this building block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually assume that given the parameters $\\omega$ (and latent factors,\n",
    "e.g. source components in mixture models), that the targets are conditionally\n",
    "independent: $\n",
    "    p(D \\mid \\omega)\n",
    "        = \\prod_i p(y_i, x_i\\mid \\omega)\n",
    "$.\n",
    "\n",
    "From these blocks under these assumptions we wish to find $\n",
    "p(\\omega\\mid D) = \\tfrac{p(D\\mid \\omega) p(\\omega)}{p(D)}\n",
    "$ -- the **posterior parameter distribution**, and having integrated\n",
    "out $\\omega$ the **posterior predictive distribution** $p(y\\mid x, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to measure difference between $q^*_D(z)$ and $p(z\\mid x)$\n",
    "(and its gradient) using only cheap operations. By assumption,\n",
    "we cant sample from $p(z\\mid x)$ or evaluate its density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can\n",
    "* evaluate density $p(x, z)$ aka unnormalized $p(z\\mid x)$\n",
    "* sample from $q^*_D(z)$ and evaluate its density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the best candidate that approximates conditional density\n",
    "of latent variables given observed variables. One notion of **proximity**\n",
    "between distributions is the KL divergence. The Kullback-Leibler divergence\n",
    "between $P$ and $Q$ with densities $p$ and $q$, respectively, is given by\n",
    "\n",
    "$$\n",
    "    \\mathrm{KL}(q(\\omega) \\| p(\\omega))\n",
    "%         = \\mathbb{E}_{\\omega \\sim Q} \\log \\tfrac{dQ}{dP}(\\omega)\n",
    "        = \\mathbb{E}_{\\omega \\sim q(\\omega)}\n",
    "            \\log \\tfrac{q(\\omega)}{p(\\omega)}\n",
    "    \\,. $$\n",
    "\n",
    "Can get unbiased estimate using only samples from $q(z\\mid x)$ and evaluations\n",
    "of $q(z\\mid x)$ and $p(z, x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Kullback-Leibler divergence of distribution $q$\n",
    "from $p$ is given by\n",
    "\n",
    "$$\n",
    "    KL(q\\| p)\n",
    "        = \\mathbb{E}_{x\\sim q(x)} \\log \\tfrac{q(x)}{p(x)}\n",
    "    \\,. $$\n",
    "\n",
    "We can compute the entropy as the Kullback-Leibler divergence\n",
    "of $p(y)$ from a uniform categorical rv:\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\mathbb{H}\\bigl(p(y)\\bigr)\n",
    "        &= - \\sum_k p_k \\log p_k\n",
    "        = - \\bigl( \\sum_k p_k \\log p_k \\pm \\log \\tfrac1d \\bigr)\n",
    "%         = - \\bigl( \\sum_k p_k \\log \\tfrac{p_k}{\\tfrac1d} \\bigr) - \\log \\tfrac1d\n",
    "        \\\\\n",
    "%         &=  \\log d - \\bigl( \\sum_k p_k \\log \\tfrac{p_k}{\\tfrac1d} \\bigr)\n",
    "        &= \\log d - KL(p\\|\\tfrac1d)\n",
    "\\end{align}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obscure -- DON'T USE\n",
    "def entropy(proba):\n",
    "    \"\"\"Compute the entropy along the last dimension.\"\"\"\n",
    "\n",
    "    ## Exercise\n",
    "    return - torch.kl_div(torch.tensor(0.).to(proba), proba).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational Inference approximation of **BALD** is given by\n",
    "the same expression, except with $q_\\theta(\\omega)$ instead of\n",
    "$p(\\omega\\mid D_\\mathrm{train})$:\n",
    "\n",
    "$$\n",
    "    \\mathbb{I}_\\mathrm{vi}(y\\,; \\omega \\mid x, D_\\mathrm{train})\n",
    "        = \\mathbb{H}\\bigl(\n",
    "            \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "                p(y\\mid x, \\omega, D_\\mathrm{train})\n",
    "        \\bigr)\n",
    "        - \\mathbb{E}_{\\omega \\sim q_\\theta(\\omega)}\n",
    "            \\mathbb{H}(p(y\\mid x, \\omega, D_\\mathrm{train}))\n",
    "    \\,. \\tag{mi-vi} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ruder.io/word-embeddings-softmax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "\n",
    "inputs = Input(shape=(1,))\n",
    "x = Dense(512, activation=\"relu\")(inputs)\n",
    "x = Dropout(0.5)(x, training=True)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x, training=True)\n",
    "outputs = Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train[:, 0], y_train[:, 0], epochs=2000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do stochastic forward passes on x_test:\n",
    "samples = [model.predict(X_test[:, 0]) for _ in range(501)]\n",
    "m = np.mean(samples, axis=0).flatten() # predictive mean\n",
    "std = np.std(samples, axis=0).flatten() # predictive variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, np.r_[samples][..., 0].T, c=\"k\", alpha=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean and uncertainty\n",
    "plt.plot(X_train, y_train, 'or')\n",
    "plt.plot(X_test, m, 'gray')\n",
    "\n",
    "plt.fill_between(X_test[:, 0], m - 2*std, m + 2*std, \n",
    "                 color='gray', alpha=0.3) # plot two std (95% confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, axis=-1):\n",
    "    return np.exp(logits - logsumexp(logits, axis=axis, keepdims=True))\n",
    "\n",
    "def entropy(proba, axis=-1):\n",
    "    out = np.multiply(proba, np.log(proba), where=proba>0,\n",
    "                      out=np.zeros_like(proba))\n",
    "\n",
    "    return -out.sum(axis=axis)\n",
    "\n",
    "def ext_h(proba, jackknife=True):\n",
    "    # broadcasting from the tail!\n",
    "    proba_avg = proba.mean(axis=0)\n",
    "\n",
    "    entropy_expected = entropy(proba_avg)\n",
    "\n",
    "    if jackknife:\n",
    "        proba_loo = proba_avg + (proba_avg - proba) / (len(proba) - 1)\n",
    "        expected_entropy_loo = entropy(proba_loo).mean(axis=0)\n",
    "        entropy_expected += (len(proba) - 1) * (entropy_expected - expected_entropy_loo)\n",
    "\n",
    "    expected_entropy = entropy(proba).mean(axis=0)\n",
    "\n",
    "    return entropy_expected, entropy_expected - expected_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob = np.r_[0.1, 0.05, 0.2, 0.15, 0.2, 0.1, 0.05, 0.05, 0.05, 0.05]\n",
    "prob = softmax(np.random.randn(20) * 1e0)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "\n",
    "ht = -sum(p * np.log(p) for p in prob if p > 0)\n",
    "\n",
    "d = dirichlet(prob*1e-1)\n",
    "\n",
    "n_replications = 1001\n",
    "for n_samples in [11, 51, 101, 1001]:\n",
    "    proba = d.rvs(size=(n_samples, n_replications))\n",
    "\n",
    "    hp, mip = ext_h(proba, False)\n",
    "    hj, mij = ext_h(proba, True)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.hist(hp, bins=21, label=\"plug-in\", alpha=0.5)\n",
    "    plt.hist(hj, bins=21, label=\"jackknife\", alpha=0.5)\n",
    "\n",
    "    plt.axvline(ht, c=\"red\")\n",
    "    plt.axvline(hp.mean(), c=\"cyan\", label=\"mean plug-in\")\n",
    "    plt.axvline(hj.mean(), c=\"olive\", label=\"mean jackknife\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
